{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "radar2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "G8Mk31W8zsVa",
        "-Zu0_8_Hr9VE",
        "7qVJDv-asHcr"
      ],
      "authorship_tag": "ABX9TyN7R0E/r8AC8J/eYXLYlG5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiheddachraoui/P2M/blob/main/radar2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUS9syJLaFo7",
        "outputId": "fd505960-8744-49c6-ec27-91753cc3b625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Cloning into 'RTCnet'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 84 (delta 37), reused 29 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (84/84), done.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!git clone https://github.com/tudelft-iv/RTCnet.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys    \n",
        "path = '/content/RTCnet'\n",
        "\n",
        "sys.path.append(path)"
      ],
      "metadata": {
        "id": "kdCEPfjraddm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate DATASET"
      ],
      "metadata": {
        "id": "G8Mk31W8zsVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as osp \n",
        "import json\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "def create_path():\n",
        "    \"\"\"\n",
        "    Create the path for the dataset.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \n",
        "    Returnes\n",
        "    --------\n",
        "    dataset_DIR: string\n",
        "                 The path to the dataset folder\n",
        "    data_DIR: string\n",
        "              The path to the data parent folder\n",
        "    \"\"\"\n",
        "    #BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "    BASE_DIR = \"/content/RTCnet\"\n",
        "    dataset_DIR = osp.join(BASE_DIR, 'dataset')\n",
        "    data_DIR = osp.join(dataset_DIR, 'data')\n",
        "    train_DIR = osp.join(data_DIR, 'train')\n",
        "    test_DIR = osp.join(data_DIR, 'test')\n",
        "    val_DIR = osp.join(data_DIR, 'val')\n",
        "    if not osp.exists(train_DIR):\n",
        "        os.makedirs(train_DIR)\n",
        "    if not osp.exists(test_DIR):\n",
        "        os.makedirs(test_DIR)\n",
        "    if not osp.exists(val_DIR):\n",
        "        os.makedirs(val_DIR)\n",
        "    return dataset_DIR, data_DIR\n",
        "\n",
        "\n",
        "def generate_dataset_single(num_features, num_samples):\n",
        "    \n",
        "    \"\"\"\n",
        "    Generate each single dataset\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_features: int\n",
        "                  The number of features for each sample\n",
        "    num_samples: int\n",
        "                 The number of samples for the dataset\n",
        "    \"\"\"\n",
        "    features_x = 120 * np.random.rand(num_samples,1) \n",
        "    features_y = 100 * np.random.rand(num_samples,1) - 50\n",
        "    features_v = 30 * np.random.rand(num_samples,1) - 15\n",
        "    features_rcs = 5 * np.random.rand(num_samples,1) \n",
        "    features_low_level = 3000 * np.random.rand(num_samples, num_features - 4)\n",
        "    features = np.concatenate([features_x, features_y, features_v, features_rcs, features_low_level], axis=1)   \n",
        "    labels = np.random.randint(4,size=(num_samples)) \n",
        "    num_targets_per_frame = 20\n",
        "    num_frames = num_samples / num_targets_per_frame\n",
        "    frame_id = np.arange(num_frames + 1)\n",
        "    frame_id = np.repeat(frame_id, num_targets_per_frame)\n",
        "    frame_id = frame_id[:num_samples]\n",
        "    instance_id = labels = np.random.randint(6,size=(num_samples)) \n",
        "\n",
        "    return features, labels, frame_id, instance_id\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_DIR, data_DIR = create_path()\n",
        "    num_features = 804\n",
        "    num_samples_train = 10000\n",
        "    num_samples_val = 1000\n",
        "    num_samples_test = 4000\n",
        "    features_train, labels_train, frame_id_train, instance_id_train  = generate_dataset_single(num_features, num_samples_train)\n",
        "    features_val, labels_val, frame_id_val, instance_id_val = generate_dataset_single(num_features, num_samples_val)\n",
        "    features_test, labels_test, frame_id_test, instance_id_test = generate_dataset_single(num_features, num_samples_test)\n",
        "    \n",
        "    \"\"\"\n",
        "    The meta data is used for configuration of the project. \n",
        "    Most attributes are related to the real data generator that generates data from real files recorded by the vehicle.\n",
        "    For example, polar_coord means whether the coordinate of radar targets will be transformed to Cartesian coordinate system.\n",
        "    \"\"\"\n",
        "    meta_data = {\n",
        "\n",
        "    \"all_mirroring\": True,\n",
        "    \"attribute\": \"all data with FOV filtering 32 cropping\",\n",
        "    \"augmentation\": True,\n",
        "    \"crop_window_size\": 5,\n",
        "    \"filter_lowlevel\": True,\n",
        "    \"mirroring\": True,\n",
        "    \"nms\": True,\n",
        "    \"polar_coord\": True,\n",
        "    \"shuffle\": True,\n",
        "    \"test_only_nearby\": False,\n",
        "    \"use_manual_annotation\": True,\n",
        "    \"view_cluster\": True,\n",
        "    \"viz_input\": False,\n",
        "    \"x_lim\": 1000,\n",
        "    \"yaw_threshold\": 100\n",
        "    }\n",
        "\n",
        "    with open(osp.join(dataset_DIR, 'meta_data.json'), 'w') as fp:\n",
        "        json.dump(meta_data, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
        "\n",
        "    np.save(osp.join(data_DIR, 'train','features.npy'), features_train)\n",
        "    np.save(osp.join(data_DIR, 'train','labels.npy'), labels_train)\n",
        "    np.save(osp.join(data_DIR, 'train','frame_id.npy'), frame_id_train)\n",
        "    np.save(osp.join(data_DIR, 'val','features.npy'), features_val)\n",
        "    np.save(osp.join(data_DIR, 'val','labels.npy'), labels_val)\n",
        "    np.save(osp.join(data_DIR, 'val','frame_id.npy'), frame_id_val)\n",
        "    np.save(osp.join(data_DIR, 'test','features.npy'), features_test)\n",
        "    np.save(osp.join(data_DIR, 'test','labels.npy'), labels_test)\n",
        "    np.save(osp.join(data_DIR, 'test','frame_id.npy'), frame_id_test)\n",
        "    np.save(osp.join(data_DIR,'instance_id_test.npy'), instance_id_test)"
      ],
      "metadata": {
        "id": "dWjuoHDuakz3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train\n"
      ],
      "metadata": {
        "id": "-Zu0_8_Hr9VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Library\n",
        "import os\n",
        "import os.path as osp \n",
        "import sys \n",
        "from time import time\n",
        "import json\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "# Third-party library\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "import easydict\n",
        "# Pytorch\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Network \n",
        "from RTCnet.RTCnet import RTCnet, TargetMLP\n",
        "from RTCnet.RTCnet_utils import Trainer\n",
        "from RTCnet.TargetLoader import TargetModeDataset, ToTensor, Permutation\n",
        "\n",
        "#BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # Base directory of the RTC module\n",
        "BASE_DIR = \"/content/RTCnet/RTCnet\"\n",
        "\n",
        "def train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class \n",
        "            ):\n",
        "    \"\"\" \n",
        "        Function to train one-over-all model\n",
        "    Args:\n",
        "        train_data (TargetModeDataset): The training dataset\n",
        "        val_data (TargetModeDataset): The validation dataset\n",
        "        train_loader (DataLoader): The dataloader for training dataset\n",
        "        val_loader (DataLoader): The dataloader for validation dataset\n",
        "        weights_all (NumpyArray): Normalized weights for classes\n",
        "        model (RTCnet): The network modules\n",
        "        optimizer (Optimizer): The optimizer in torch.optim\n",
        "        scheduler: The scheduler for training\n",
        "        eval_frequency (int): The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "        use_gpu (bool): Whether using GPU for training and validation\n",
        "        lr_decay_step (int): the number of steps for learning rate decay\n",
        "        decay_f (double): the decay factor of learning rate\n",
        "        lr_clip (double): the clip of learning rate during decay process\n",
        "        result_folder (string): the folder to save the training result\n",
        "        n_epochs (int): the number of epochs for training\n",
        "        chose_class (int): the class number for the chosen class  \n",
        "\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs All\".format(class_list[chose_class]))\n",
        "    append_str = \"{}_vs_All\".format(class_list[chose_class])\n",
        "    weights = torch.tensor(np.array([np.sum(weights_all)-weights_all[chose_class], weights_all[chose_class]]))\n",
        "    if use_gpu:\n",
        "        weights = weights.cuda()\n",
        "    print(\"weights:{}\".format(weights))\n",
        "    loss_func = nn.CrossEntropyLoss(weight = weights)\n",
        "    #################### data ##########################\n",
        "    train_data.to_ova(chose_class = chose_class)\n",
        "    val_data.to_ova(chose_class = chose_class)\n",
        "    ################### Trainer ##########################\n",
        "    trainer = Trainer(\n",
        "                model, \n",
        "                loss_func,\n",
        "                optimizer,\n",
        "                lr_scheduler = scheduler,\n",
        "                eval_frequency = eval_frequency,\n",
        "                use_gpu = use_gpu,\n",
        "                lr_decay_step=lr_decay_step,\n",
        "                lr_decay_f=decay_f,\n",
        "                lr_clip=lr_clip,\n",
        "                save_checkerpoint_to=result_folder,\n",
        "                append_str= append_str\n",
        "    )\n",
        "    trainer.train(\n",
        "            n_epochs,\n",
        "            train_loader,\n",
        "            val_loader=val_loader,\n",
        "            best_loss=1e5,\n",
        "            start_it=0\n",
        "    )\n",
        "    loss_trajectory = trainer.trace_loss\n",
        "    loss_trajectory_train = trainer.trace_loss_train\n",
        "    np.save(osp.join(result_folder, \"loss_trajectory\" + append_str), loss_trajectory)\n",
        "    np.save(osp.join(result_folder, \"train_loss_trajectory\" + append_str), loss_trajectory_train)\n",
        "\n",
        "def train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            class_positive,\n",
        "            class_negative\n",
        "            ):\n",
        "    \"\"\" \n",
        "        Function to train one-over-one model\n",
        "\n",
        "    Args:\n",
        "        train_data (TargetModeDataset): The training dataset\n",
        "        val_data (TargetModeDataset): The validation dataset\n",
        "        train_loader (DataLoader): The dataloader for training dataset\n",
        "        val_loader (DataLoader): The dataloader for validation dataset\n",
        "        weights_all (NumpyArray): Normalized weights for classes\n",
        "        model (RTCnet): The network modules\n",
        "        optimizer (Optimizer): The optimizer in torch.optim\n",
        "        scheduler: The scheduler for training\n",
        "        eval_frequency (int): The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "        use_gpu (bool): Whether using GPU for training and validation\n",
        "        lr_decay_step (int): the number of steps for learning rate decay\n",
        "        decay_f (double): the decay factor of learning rate\n",
        "        lr_clip (double): the clip of learning rate during decay process\n",
        "        result_folder (string): the folder to save the training result\n",
        "        n_epochs (int): the number of epochs for training\n",
        "        class_positive(int): the class number for the positive class\n",
        "        class_negative(int): the class number for the negative class\n",
        "\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs {}\".format(class_list[class_positive], class_list[class_negative]))\n",
        "    append_str = \"{}_vs_{}\".format(class_list[class_positive], class_list[class_negative])\n",
        "    weights = torch.tensor(np.array([weights_all[class_negative], weights_all[class_positive]]))\n",
        "    if use_gpu:\n",
        "        weights = weights.cuda()\n",
        "    print(\"weights:{}\".format(weights))\n",
        "    loss_func = nn.CrossEntropyLoss(weight = weights)\n",
        "    #################### data ##########################\n",
        "    train_data.to_ovo(class_positive, class_negative)\n",
        "    val_data.to_ovo(class_positive, class_negative)\n",
        "    ################### Trainer ##########################\n",
        "    trainer = Trainer(\n",
        "                model, \n",
        "                loss_func,\n",
        "                optimizer,\n",
        "                lr_scheduler = scheduler,\n",
        "                eval_frequency = eval_frequency,\n",
        "                use_gpu = use_gpu,\n",
        "                lr_decay_step=lr_decay_step,\n",
        "                lr_decay_f=decay_f,\n",
        "                lr_clip=lr_clip,\n",
        "                save_checkerpoint_to=result_folder,\n",
        "                append_str= append_str\n",
        "    )\n",
        "    trainer.train(\n",
        "            n_epochs,\n",
        "            train_loader,\n",
        "            val_loader=val_loader,\n",
        "            best_loss=1e5,\n",
        "            start_it=0\n",
        "    )\n",
        "    loss_trajectory = trainer.trace_loss\n",
        "    np.save(osp.join(result_folder, \"loss_trajectory\" + append_str), loss_trajectory)\n",
        "    loss_trajectory_train = trainer.trace_loss_train\n",
        "    np.save(osp.join(result_folder, \"train_loss_trajectory\" + append_str), loss_trajectory_train)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    '''parser.add_argument(\"--data\", type=str, default = osp.join(BASE_DIR, osp.pardir, 'dataset','data'), help = \"The data path that contains train, val and test folder, this is generated by gen_input_data_and_baseline.py\" )\n",
        "    parser.add_argument(\"--batch_size\", type=int, default = 1024, help = \"batch size for training\" ) \n",
        "    parser.add_argument(\"--rm_speed\", type=bool, default = False, help = \"Whether to remove the speed during training\" )\n",
        "    parser.add_argument(\"--rm_rcs\", type=bool, default = False, help = \"Whether to remove the RCS value during training\" ) \n",
        "    parser.add_argument(\"--n_epochs\", type=int, default = 1, help = \"number of epochs\" ) '''\n",
        "    #args            = parser.parse_args()\n",
        "    #args = parser.parse_args(argv[1:])\n",
        "    args = easydict.EasyDict({\n",
        "        \"data\": '/content/RTCnet/dataset/data',\n",
        "        \"batch_size\": 1024,\n",
        "        \"rm_speed\": False,\n",
        "        \"rm_rcs\": False,\n",
        "        \"n_epochs\": 1\n",
        "    \n",
        "})\n",
        "    data_path       = args.data\n",
        "    rm_speed        = args.rm_speed\n",
        "    rm_rcs          = args.rm_rcs \n",
        "    n_epochs        = args.n_epochs\n",
        "    model_version   = 4\n",
        "    chosen_feature_type = 'high' if model_version == 3 else 'low' \n",
        "\n",
        "    # Get the meta information of the dataset\n",
        "    data_meta_dir   = osp.join(data_path, os.pardir)\n",
        "    data_meta_path  = osp.join(data_meta_dir, 'meta_data.json')\n",
        "    with open(data_meta_path) as fp:\n",
        "        data_meta = json.load(fp)\n",
        "    data_x_lim      = data_meta['x_lim']                    # The maximum limit for the longitudinal distance\n",
        "\n",
        "    # Training setup\n",
        "    lr_start        = 1e-3                                  # The leraning rate starts from lr_start, but it decays according to certain policies\n",
        "    eval_frequency  = 0                                     # The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "    batch_size      = args.batch_size                       # The batch size for stochastic gradient descent\n",
        "    lr_decay_step   = 2000                                  # The step after which the learning rate is decayed. But if the optimizer is used, this is useless.\n",
        "    decay_f         = 0.9                                   # The decay factor for the learning rate. After each lr_decay_step iterations, the learning rate is equal to decay_f * last leraning rate\n",
        "    lr_clip         = 2e-4                                  # The clip of learning rate. The learning rate stops decaying after this value\n",
        "    use_gpu         = True                                  # Whether use gpu to train\n",
        "    dropout         = True\n",
        "    if_shuffle      = True                                  # Whether to shuffle the data when loading them. This should always be true when training\n",
        "    speed_limit     = 0                                     # The speed limit used when loading data on the fly. But if the data is already filtered when generated, this can be set as 0\n",
        "    high_dims       = 4                                     # The number of dimensions for high-level features\n",
        "    dist_near       = 0                                     # The nearest distance threshold. If using polar coordinate, this is range rather than distance\n",
        "    binary_class    = False                                 # If using binary class\n",
        "    use_weight      = True                                  # If using weights for loss function\n",
        "    weights_factor  = np.array([1/0.5, 1/1, 1/0.5, 1/1])    # The scaling factor for the weights, considering unbalanced classes\n",
        "    rm_cars         = False                                 # Whether to remove cars when training\n",
        "    input_size      = 5                                     # The input size of RTC2 window\n",
        "    rm_position     = False                                 # Whether to remove position during training\n",
        "\n",
        "    t = int(time())\n",
        "    result_parent_path = osp.join(BASE_DIR, osp.pardir, 'results')\n",
        "\n",
        "    result_folder = osp.join(result_parent_path,\"RTCresults\", str(t))\n",
        "    train_info_path = osp.join(result_parent_path, 'RTCtrain_info')\n",
        "\n",
        "    if not osp.exists(train_info_path):\n",
        "        os.makedirs(train_info_path)\n",
        "    if not osp.exists(result_folder):\n",
        "        os.makedirs(result_folder)\n",
        "\n",
        "    ################### transforms ###################\n",
        "    to_tensor = ToTensor()\n",
        "    perm_position = Permutation()\n",
        "    composed_trans = transforms.Compose([perm_position, to_tensor])\n",
        "\n",
        "    ################### data loader ###################  \n",
        "    ## Train dataset \n",
        "    train_data = TargetModeDataset(\n",
        "                data_path, composed_trans, \n",
        "                mode='train', high_dims = high_dims, \n",
        "                normalize = True, feature_type = chosen_feature_type,\n",
        "                norms_path=result_folder,\n",
        "                speed_limit=speed_limit,\n",
        "                dist_near= dist_near,\n",
        "                binary_class = binary_class,\n",
        "                dist_far=data_x_lim,\n",
        "                rm_cars=rm_cars,\n",
        "                rm_position=rm_position,\n",
        "                rm_speed = rm_speed,\n",
        "                rm_rcs=rm_rcs)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=if_shuffle, num_workers=2)\n",
        "    ## Validation dataset\n",
        "    val_data = TargetModeDataset(\n",
        "                data_path, composed_trans, \n",
        "                mode='val', high_dims=high_dims, \n",
        "                normalize=True, feature_type= chosen_feature_type,\n",
        "                norms_path=result_folder,\n",
        "                speed_limit=speed_limit,\n",
        "                dist_near= dist_near,\n",
        "                binary_class = binary_class,\n",
        "                dist_far=data_x_lim,\n",
        "                rm_cars=rm_cars,\n",
        "                rm_position=rm_position,\n",
        "                rm_speed = rm_speed,\n",
        "                rm_rcs=rm_rcs)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    train_data.save_features_labels()\n",
        "    val_data.save_features_labels()\n",
        "\n",
        "    ################### Model ################### \n",
        "\n",
        "    if model_version == 4:\n",
        "        model = RTCnet(\n",
        "                num_classes=2, \n",
        "                Doppler_dims=32, \n",
        "                high_dims = high_dims, \n",
        "                dropout= dropout,\n",
        "                input_size = input_size)\n",
        "    elif model_version == 3:\n",
        "        model = TargetMLP(\n",
        "                num_classes=2,\n",
        "                high_dims=high_dims)\n",
        "\n",
        "    model.double()\n",
        "\n",
        "\n",
        "    ################### weight initialization ################### \n",
        "    if use_weight:\n",
        "        num_others = np.sum(train_data.labels==0)\n",
        "        num_ped = np.sum(train_data.labels == 1)\n",
        "        num_bikers = np.sum(train_data.labels==2)\n",
        "        num_car = np.sum(train_data.labels==3)\n",
        "        print(\"num_others:{}, num_peds:{}, num_bikers:{}, num_car:{}\".format(num_others, num_ped, num_bikers, num_car))\n",
        "\n",
        "        weights = np.array([1/num_others,1/num_ped,1/num_bikers,1/num_car])\n",
        "    else:\n",
        "        weights = np.ones(4)\n",
        "\n",
        "    weights = np.multiply(weights_factor, weights)\n",
        "    weights_save = deepcopy(weights.tolist())\n",
        "    weights_all = np.divide(weights, np.sum(weights))\n",
        "\n",
        "    ################### Save the training information #############################\n",
        "    train_info = {\n",
        "    \"data_path\": data_path, \n",
        "    \"lr_start\": lr_start,\n",
        "    \"n_epochs\": n_epochs,\n",
        "    \"eval_frequency\": eval_frequency,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"lr_decay_step\":lr_decay_step,\n",
        "    \"decay_f\":decay_f,\n",
        "    \"lr_clip\":lr_clip,\n",
        "    \"use_gpu\":use_gpu,\n",
        "    \"t\":t,\n",
        "    \"result_folder\":result_folder,\n",
        "    \"weights\": weights_save,\n",
        "    \"weights_factor\": weights_factor.tolist(),\n",
        "    \"if_shuffle\":if_shuffle,\n",
        "    \"dropout\":dropout,\n",
        "    \"speed_limit\": speed_limit,\n",
        "    \"dist_limit\": dist_near,\n",
        "    \"binary\": binary_class,\n",
        "    \"input_size\": input_size,\n",
        "    \"model_version\": model_version,\n",
        "    \"data_x_limit\": data_x_lim,\n",
        "    \"use_weight\":use_weight,\n",
        "    \"rm_cars\": rm_cars,\n",
        "    \"rm_position\": rm_position,\n",
        "    \"rm_speed\": rm_speed,\n",
        "    \"rm_rcs\": rm_rcs\n",
        "    }\n",
        "    with open(osp.join(train_info_path, '{}.json'.format(t)), 'w') as fp:\n",
        "        print(\"save the file at :{}\".format(osp.join(train_info_path, '{}.json'.format(t))))\n",
        "        json.dump(train_info, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################### Optimizer ########################\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_start)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=lr_decay_step,gamma=0.1)\n",
        "    \n",
        "    ################### Ped vs ALL ########################\n",
        "    \n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 1 \n",
        "            )\n",
        "\n",
        "    ################### Biker VS ALL ######################\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 2\n",
        "    ) \n",
        "    ################### Car VS ALL ######################\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 3\n",
        "    ) \n",
        "    ################### others VS ALL ######################\n",
        "\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 0\n",
        "    ) \n",
        "\n",
        "    ################### Ped VS biker ########################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            1,\n",
        "            2\n",
        "            )\n",
        "\n",
        "    #################### Ped VS car #########################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            1,\n",
        "            3\n",
        "            )\n",
        "    ################### Biker VS car #######################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            2,\n",
        "            3\n",
        "            )\n",
        "    #################### Others VS ped #####################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            1\n",
        "            )\n",
        "    #################### Others VS biker #####################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            2\n",
        "            )\n",
        "    ##################### Others VS car #######################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            3\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9UemTrDbjVn",
        "outputId": "9404718c-6216-4972-a8db-57aba22d96a0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train feature shape:(10000, 804) \n",
            "train features after removing static targets:(10000, 804)\n",
            "train features after removing neaby and far away targets:(10000, 804)\n",
            "val feature shape:(1000, 804) \n",
            "val features after removing static targets:(1000, 804)\n",
            "val features after removing neaby and far away targets:(1000, 804)\n",
            "num_others:1664, num_peds:1642, num_bikers:1643, num_car:1666\n",
            "save the file at :/content/RTCnet/RTCnet/../results/RTCtrain_info/1651631446.json\n",
            "start training ped vs All\n",
            "weights:tensor([0.8322, 0.1678], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.6900705661000153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.35it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  5.04it/s, total_it=1, loss=0.68]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  6.12it/s, total_it=2, loss=0.64]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.78it/s, total_it=3, loss=0.60]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  7.24it/s, total_it=4, loss=0.56]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.53it/s, total_it=5, loss=0.50]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.72it/s, total_it=6, loss=0.45]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.92it/s, total_it=7, loss=0.40]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  8.01it/s, total_it=8, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.44it/s, total_it=9, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.5467346695689691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs All\n",
            "weights:tensor([0.6645, 0.3355], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.5633530263150165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.34it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  5.15it/s, total_it=1, loss=0.30]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  6.17it/s, total_it=2, loss=0.38]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.85it/s, total_it=3, loss=0.32]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  7.23it/s, total_it=4, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.49it/s, total_it=5, loss=0.37]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.68it/s, total_it=6, loss=0.39]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.84it/s, total_it=7, loss=0.36]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  7.94it/s, total_it=8, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.37it/s, total_it=9, loss=0.36]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.31582630727244265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
            "\n",
            "                                                                     \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training car vs All\n",
            "weights:tensor([0.8346, 0.1654], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.22418418408695306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.26it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  5.10it/s, total_it=1, loss=0.16]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  6.15it/s, total_it=2, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.72it/s, total_it=3, loss=0.18]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  7.18it/s, total_it=4, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.45it/s, total_it=5, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.65it/s, total_it=6, loss=0.18]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.82it/s, total_it=7, loss=0.18]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  7.92it/s, total_it=8, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.35it/s, total_it=9, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.16019026849734908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
            "\n",
            "                                                                     \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs All\n",
            "weights:tensor([0.6688, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.3287163569364652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.30it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  4.98it/s, total_it=1, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  5.98it/s, total_it=2, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.72it/s, total_it=3, loss=0.33]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  7.18it/s, total_it=4, loss=0.32]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.50it/s, total_it=5, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.72it/s, total_it=6, loss=0.33]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.87it/s, total_it=7, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  7.92it/s, total_it=8, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.39it/s, total_it=9, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.3209261521385472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs biker\n",
            "weights:tensor([0.3355, 0.1678], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.9008115900898532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.57it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.39it/s, total_it=1, loss=0.91]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.38it/s, total_it=2, loss=0.83]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7368909418948957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
            "\n",
            "                                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs car\n",
            "weights:tensor([0.1654, 0.1678], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.9990303119987269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.62it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.41it/s, total_it=1, loss=0.94]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.38it/s, total_it=2, loss=0.87]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.795547777161637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs car\n",
            "weights:tensor([0.1654, 0.3355], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.8902082291592448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.58it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.31it/s, total_it=1, loss=0.92]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.29it/s, total_it=2, loss=0.86]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7618507985975651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs ped\n",
            "weights:tensor([0.1678, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.7623729464865884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.26it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.09it/s, total_it=1, loss=0.78]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.16it/s, total_it=2, loss=0.76]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7181713392907335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs biker\n",
            "weights:tensor([0.3355, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.6999500173199713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.39it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.18it/s, total_it=1, loss=0.69]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.19it/s, total_it=2, loss=0.69]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.6954777570067174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs car\n",
            "weights:tensor([0.1654, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.7058124868004496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.31it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.12it/s, total_it=1, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.16it/s, total_it=2, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.6994545937823294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
            "\n",
            "                                                                  \u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "7qVJDv-asHcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Library\n",
        "import os\n",
        "import os.path as osp \n",
        "import sys \n",
        "from time import time\n",
        "import json\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "import easydict\n",
        "\n",
        "# Third-party library\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "\n",
        "# Pytorch\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Network \n",
        "from RTCnet.RTCnet import RTCnet, TargetMLP\n",
        "from RTCnet.RTCnet_utils import Trainer\n",
        "from RTCnet.TargetLoader import TargetModeDataset, ToTensor, Permutation\n",
        "\n",
        "#BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # Base directory of the RTC module\n",
        "BASE_DIR = \"/content/RTCnet/RTCnet\"\n",
        "\n",
        "def train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class \n",
        "            ):\n",
        "    \"\"\" \n",
        "        Function to train one-over-all model\n",
        "    Args:\n",
        "        train_data (TargetModeDataset): The training dataset\n",
        "        val_data (TargetModeDataset): The validation dataset\n",
        "        train_loader (DataLoader): The dataloader for training dataset\n",
        "        val_loader (DataLoader): The dataloader for validation dataset\n",
        "        weights_all (NumpyArray): Normalized weights for classes\n",
        "        model (RTCnet): The network modules\n",
        "        optimizer (Optimizer): The optimizer in torch.optim\n",
        "        scheduler: The scheduler for training\n",
        "        eval_frequency (int): The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "        use_gpu (bool): Whether using GPU for training and validation\n",
        "        lr_decay_step (int): the number of steps for learning rate decay\n",
        "        decay_f (double): the decay factor of learning rate\n",
        "        lr_clip (double): the clip of learning rate during decay process\n",
        "        result_folder (string): the folder to save the training result\n",
        "        n_epochs (int): the number of epochs for training\n",
        "        chose_class (int): the class number for the chosen class  \n",
        "\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs All\".format(class_list[chose_class]))\n",
        "    append_str = \"{}_vs_All\".format(class_list[chose_class])\n",
        "    weights = torch.tensor(np.array([np.sum(weights_all)-weights_all[chose_class], weights_all[chose_class]]))\n",
        "    if use_gpu:\n",
        "        weights = weights.cuda()\n",
        "    print(\"weights:{}\".format(weights))\n",
        "    loss_func = nn.CrossEntropyLoss(weight = weights)\n",
        "    #################### data ##########################\n",
        "    train_data.to_ova(chose_class = chose_class)\n",
        "    val_data.to_ova(chose_class = chose_class)\n",
        "    ################### Trainer ##########################\n",
        "    trainer = Trainer(\n",
        "                model, \n",
        "                loss_func,\n",
        "                optimizer,\n",
        "                lr_scheduler = scheduler,\n",
        "                eval_frequency = eval_frequency,\n",
        "                use_gpu = use_gpu,\n",
        "                lr_decay_step=lr_decay_step,\n",
        "                lr_decay_f=decay_f,\n",
        "                lr_clip=lr_clip,\n",
        "                save_checkerpoint_to=result_folder,\n",
        "                append_str= append_str\n",
        "    )\n",
        "    trainer.train(\n",
        "            n_epochs,\n",
        "            train_loader,\n",
        "            val_loader=val_loader,\n",
        "            best_loss=1e5,\n",
        "            start_it=0\n",
        "    )\n",
        "    loss_trajectory = trainer.trace_loss\n",
        "    loss_trajectory_train = trainer.trace_loss_train\n",
        "    np.save(osp.join(result_folder, \"loss_trajectory\" + append_str), loss_trajectory)\n",
        "    np.save(osp.join(result_folder, \"train_loss_trajectory\" + append_str), loss_trajectory_train)\n",
        "\n",
        "def train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            class_positive,\n",
        "            class_negative\n",
        "            ):\n",
        "    \"\"\" \n",
        "        Function to train one-over-one model\n",
        "\n",
        "    Args:\n",
        "        train_data (TargetModeDataset): The training dataset\n",
        "        val_data (TargetModeDataset): The validation dataset\n",
        "        train_loader (DataLoader): The dataloader for training dataset\n",
        "        val_loader (DataLoader): The dataloader for validation dataset\n",
        "        weights_all (NumpyArray): Normalized weights for classes\n",
        "        model (RTCnet): The network modules\n",
        "        optimizer (Optimizer): The optimizer in torch.optim\n",
        "        scheduler: The scheduler for training\n",
        "        eval_frequency (int): The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "        use_gpu (bool): Whether using GPU for training and validation\n",
        "        lr_decay_step (int): the number of steps for learning rate decay\n",
        "        decay_f (double): the decay factor of learning rate\n",
        "        lr_clip (double): the clip of learning rate during decay process\n",
        "        result_folder (string): the folder to save the training result\n",
        "        n_epochs (int): the number of epochs for training\n",
        "        class_positive(int): the class number for the positive class\n",
        "        class_negative(int): the class number for the negative class\n",
        "\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs {}\".format(class_list[class_positive], class_list[class_negative]))\n",
        "    append_str = \"{}_vs_{}\".format(class_list[class_positive], class_list[class_negative])\n",
        "    weights = torch.tensor(np.array([weights_all[class_negative], weights_all[class_positive]]))\n",
        "    if use_gpu:\n",
        "        weights = weights.cuda()\n",
        "    print(\"weights:{}\".format(weights))\n",
        "    loss_func = nn.CrossEntropyLoss(weight = weights)\n",
        "    #################### data ##########################\n",
        "    train_data.to_ovo(class_positive, class_negative)\n",
        "    val_data.to_ovo(class_positive, class_negative)\n",
        "    ################### Trainer ##########################\n",
        "    trainer = Trainer(\n",
        "                model, \n",
        "                loss_func,\n",
        "                optimizer,\n",
        "                lr_scheduler = scheduler,\n",
        "                eval_frequency = eval_frequency,\n",
        "                use_gpu = use_gpu,\n",
        "                lr_decay_step=lr_decay_step,\n",
        "                lr_decay_f=decay_f,\n",
        "                lr_clip=lr_clip,\n",
        "                save_checkerpoint_to=result_folder,\n",
        "                append_str= append_str\n",
        "    )\n",
        "    trainer.train(\n",
        "            n_epochs,\n",
        "            train_loader,\n",
        "            val_loader=val_loader,\n",
        "            best_loss=1e5,\n",
        "            start_it=0\n",
        "    )\n",
        "    loss_trajectory = trainer.trace_loss\n",
        "    np.save(osp.join(result_folder, \"loss_trajectory\" + append_str), loss_trajectory)\n",
        "    loss_trajectory_train = trainer.trace_loss_train\n",
        "    np.save(osp.join(result_folder, \"train_loss_trajectory\" + append_str), loss_trajectory_train)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    '''parser.add_argument(\"--data\", type=str, default = osp.join(BASE_DIR, osp.pardir, 'dataset','data'), help = \"The data path that contains train, val and test folder, this is generated by gen_input_data_and_baseline.py\" )\n",
        "    parser.add_argument(\"--batch_size\", type=int, default = 1024, help = \"batch size for training\" ) \n",
        "    parser.add_argument(\"--rm_speed\", type=bool, default = False, help = \"Whether to remove the speed during training\" )\n",
        "    parser.add_argument(\"--rm_rcs\", type=bool, default = False, help = \"Whether to remove the RCS value during training\" ) \n",
        "    parser.add_argument(\"--n_epochs\", type=int, default = 1, help = \"number of epochs\" ) \n",
        "    args            = parser.parse_args()'''\n",
        "    args = easydict.EasyDict({\"data\": '/content/RTCnet/dataset/data',\"batch_size\": 1024,\"rm_speed\": False,\"rm_rcs\": False,\n",
        "    \"n_epochs\": 1\n",
        "    \n",
        "})\n",
        "    data_path       = args.data\n",
        "    rm_speed        = args.rm_speed\n",
        "    rm_rcs          = args.rm_rcs \n",
        "    n_epochs        = args.n_epochs\n",
        "    model_version   = 4\n",
        "    chosen_feature_type = 'high' if model_version == 3 else 'low' \n",
        "\n",
        "    # Get the meta information of the dataset\n",
        "    data_meta_dir   = osp.join(data_path, os.pardir)\n",
        "    data_meta_path  = osp.join(data_meta_dir, 'meta_data.json')\n",
        "    with open(data_meta_path) as fp:\n",
        "        data_meta = json.load(fp)\n",
        "    data_x_lim      = data_meta['x_lim']                    # The maximum limit for the longitudinal distance\n",
        "\n",
        "    # Training setup\n",
        "    lr_start        = 1e-3                                  # The leraning rate starts from lr_start, but it decays according to certain policies\n",
        "    eval_frequency  = 0                                     # The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "    batch_size      = args.batch_size                       # The batch size for stochastic gradient descent\n",
        "    lr_decay_step   = 2000                                  # The step after which the learning rate is decayed. But if the optimizer is used, this is useless.\n",
        "    decay_f         = 0.9                                   # The decay factor for the learning rate. After each lr_decay_step iterations, the learning rate is equal to decay_f * last leraning rate\n",
        "    lr_clip         = 2e-4                                  # The clip of learning rate. The learning rate stops decaying after this value\n",
        "    use_gpu         = True                                  # Whether use gpu to train\n",
        "    dropout         = True\n",
        "    if_shuffle      = True                                  # Whether to shuffle the data when loading them. This should always be true when training\n",
        "    speed_limit     = 0                                     # The speed limit used when loading data on the fly. But if the data is already filtered when generated, this can be set as 0\n",
        "    high_dims       = 4                                     # The number of dimensions for high-level features\n",
        "    dist_near       = 0                                     # The nearest distance threshold. If using polar coordinate, this is range rather than distance\n",
        "    binary_class    = False                                 # If using binary class\n",
        "    use_weight      = True                                  # If using weights for loss function\n",
        "    weights_factor  = np.array([1/0.5, 1/1, 1/0.5, 1/1])    # The scaling factor for the weights, considering unbalanced classes\n",
        "    rm_cars         = False                                 # Whether to remove cars when training\n",
        "    input_size      = 5                                     # The input size of RTC2 window\n",
        "    rm_position     = False                                 # Whether to remove position during training\n",
        "\n",
        "    t = int(time())\n",
        "    result_parent_path = osp.join(BASE_DIR, osp.pardir, 'results')\n",
        "\n",
        "    result_folder = osp.join(result_parent_path,\"RTCresults\", str(t))\n",
        "    train_info_path = osp.join(result_parent_path, 'RTCtrain_info')\n",
        "\n",
        "    if not osp.exists(train_info_path):\n",
        "        os.makedirs(train_info_path)\n",
        "    if not osp.exists(result_folder):\n",
        "        os.makedirs(result_folder)\n",
        "\n",
        "    ################### transforms ###################\n",
        "    to_tensor = ToTensor()\n",
        "    perm_position = Permutation()\n",
        "    composed_trans = transforms.Compose([perm_position, to_tensor])\n",
        "\n",
        "    ################### data loader ###################  \n",
        "    ## Train dataset \n",
        "    train_data = TargetModeDataset(\n",
        "                data_path, composed_trans, \n",
        "                mode='train', high_dims = high_dims, \n",
        "                normalize = True, feature_type = chosen_feature_type,\n",
        "                norms_path=result_folder,\n",
        "                speed_limit=speed_limit,\n",
        "                dist_near= dist_near,\n",
        "                binary_class = binary_class,\n",
        "                dist_far=data_x_lim,\n",
        "                rm_cars=rm_cars,\n",
        "                rm_position=rm_position,\n",
        "                rm_speed = rm_speed,\n",
        "                rm_rcs=rm_rcs)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=if_shuffle, num_workers=2)\n",
        "    ## Validation dataset\n",
        "    val_data = TargetModeDataset(\n",
        "                data_path, composed_trans, \n",
        "                mode='val', high_dims=high_dims, \n",
        "                normalize=True, feature_type= chosen_feature_type,\n",
        "                norms_path=result_folder,\n",
        "                speed_limit=speed_limit,\n",
        "                dist_near= dist_near,\n",
        "                binary_class = binary_class,\n",
        "                dist_far=data_x_lim,\n",
        "                rm_cars=rm_cars,\n",
        "                rm_position=rm_position,\n",
        "                rm_speed = rm_speed,\n",
        "                rm_rcs=rm_rcs)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    train_data.save_features_labels()\n",
        "    val_data.save_features_labels()\n",
        "\n",
        "    ################### Model ################### \n",
        "\n",
        "    if model_version == 4:\n",
        "        model = RTCnet(\n",
        "                num_classes=2, \n",
        "                Doppler_dims=32, \n",
        "                high_dims = high_dims, \n",
        "                dropout= dropout,\n",
        "                input_size = input_size)\n",
        "    elif model_version == 3:\n",
        "        model = TargetMLP(\n",
        "                num_classes=2,\n",
        "                high_dims=high_dims)\n",
        "\n",
        "    model.double()\n",
        "\n",
        "\n",
        "    ################### weight initialization ################### \n",
        "    if use_weight:\n",
        "        num_others = np.sum(train_data.labels==0)\n",
        "        num_ped = np.sum(train_data.labels == 1)\n",
        "        num_bikers = np.sum(train_data.labels==2)\n",
        "        num_car = np.sum(train_data.labels==3)\n",
        "        print(\"num_others:{}, num_peds:{}, num_bikers:{}, num_car:{}\".format(num_others, num_ped, num_bikers, num_car))\n",
        "\n",
        "        weights = np.array([1/num_others,1/num_ped,1/num_bikers,1/num_car])\n",
        "    else:\n",
        "        weights = np.ones(4)\n",
        "\n",
        "    weights = np.multiply(weights_factor, weights)\n",
        "    weights_save = deepcopy(weights.tolist())\n",
        "    weights_all = np.divide(weights, np.sum(weights))\n",
        "\n",
        "    ################### Save the training information #############################\n",
        "    train_info = {\n",
        "    \"data_path\": data_path, \n",
        "    \"lr_start\": lr_start,\n",
        "    \"n_epochs\": n_epochs,\n",
        "    \"eval_frequency\": eval_frequency,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"lr_decay_step\":lr_decay_step,\n",
        "    \"decay_f\":decay_f,\n",
        "    \"lr_clip\":lr_clip,\n",
        "    \"use_gpu\":use_gpu,\n",
        "    \"t\":t,\n",
        "    \"result_folder\":result_folder,\n",
        "    \"weights\": weights_save,\n",
        "    \"weights_factor\": weights_factor.tolist(),\n",
        "    \"if_shuffle\":if_shuffle,\n",
        "    \"dropout\":dropout,\n",
        "    \"speed_limit\": speed_limit,\n",
        "    \"dist_limit\": dist_near,\n",
        "    \"binary\": binary_class,\n",
        "    \"input_size\": input_size,\n",
        "    \"model_version\": model_version,\n",
        "    \"data_x_limit\": data_x_lim,\n",
        "    \"use_weight\":use_weight,\n",
        "    \"rm_cars\": rm_cars,\n",
        "    \"rm_position\": rm_position,\n",
        "    \"rm_speed\": rm_speed,\n",
        "    \"rm_rcs\": rm_rcs\n",
        "    }\n",
        "    with open(osp.join(train_info_path, '{}.json'.format(t)), 'w') as fp:\n",
        "        print(\"save the file at :{}\".format(osp.join(train_info_path, '{}.json'.format(t))))\n",
        "        json.dump(train_info, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################### Optimizer ########################\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_start)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=lr_decay_step,gamma=0.1)\n",
        "    \n",
        "    ################### Ped vs ALL ########################\n",
        "    \n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 1 \n",
        "            )\n",
        "\n",
        "    ################### Biker VS ALL ######################\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 2\n",
        "    ) \n",
        "    ################### Car VS ALL ######################\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 3\n",
        "    ) \n",
        "    ################### others VS ALL ######################\n",
        "\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 0\n",
        "    ) \n",
        "\n",
        "    ################### Ped VS biker ########################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            1,\n",
        "            2\n",
        "            )\n",
        "\n",
        "    #################### Ped VS car #########################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            1,\n",
        "            3\n",
        "            )\n",
        "    ################### Biker VS car #######################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            2,\n",
        "            3\n",
        "            )\n",
        "    #################### Others VS ped #####################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            1\n",
        "            )\n",
        "    #################### Others VS biker #####################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            2\n",
        "            )\n",
        "    ##################### Others VS car #######################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            3\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R53WHZ-mbqZW",
        "outputId": "9417c506-5d3a-4413-bfd9-d101778aec6e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train feature shape:(10000, 804) \n",
            "train features after removing static targets:(10000, 804)\n",
            "train features after removing neaby and far away targets:(10000, 804)\n",
            "val feature shape:(1000, 804) \n",
            "val features after removing static targets:(1000, 804)\n",
            "val features after removing neaby and far away targets:(1000, 804)\n",
            "num_others:1664, num_peds:1642, num_bikers:1643, num_car:1666\n",
            "save the file at :/content/RTCnet/RTCnet/../results/RTCtrain_info/1651631502.json\n",
            "start training ped vs All\n",
            "weights:tensor([0.8322, 0.1678], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.6892982670580089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.22it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  4.98it/s, total_it=1, loss=0.66]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  5.93it/s, total_it=2, loss=0.62]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.64it/s, total_it=3, loss=0.57]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  7.09it/s, total_it=4, loss=0.51]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.40it/s, total_it=5, loss=0.46]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.59it/s, total_it=6, loss=0.41]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.70it/s, total_it=7, loss=0.36]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  7.75it/s, total_it=8, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.18it/s, total_it=9, loss=0.28]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.32it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.5157576902140203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs All\n",
            "weights:tensor([0.6645, 0.3355], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.5361638838140471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.10it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  4.83it/s, total_it=1, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  5.84it/s, total_it=2, loss=0.32]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.52it/s, total_it=3, loss=0.33]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  6.93it/s, total_it=4, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.28it/s, total_it=5, loss=0.39]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.52it/s, total_it=6, loss=0.37]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.69it/s, total_it=7, loss=0.39]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  7.81it/s, total_it=8, loss=0.40]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.21it/s, total_it=9, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.31958731319627837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training car vs All\n",
            "weights:tensor([0.8346, 0.1654], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.23182016419136164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.26it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  4.88it/s, total_it=1, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  5.94it/s, total_it=2, loss=0.17]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.63it/s, total_it=3, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  7.03it/s, total_it=4, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.35it/s, total_it=5, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.56it/s, total_it=6, loss=0.18]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.69it/s, total_it=7, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  7.76it/s, total_it=8, loss=0.18]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.13it/s, total_it=9, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.24it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.1674972041001531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs All\n",
            "weights:tensor([0.6688, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.32023480442507934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:00<00:02,  3.28it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:00<00:01,  4.96it/s, total_it=1, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:00<00:01,  5.92it/s, total_it=2, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:00<00:00,  6.65it/s, total_it=3, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:00<00:00,  7.05it/s, total_it=4, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:00<00:00,  7.31it/s, total_it=5, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:01<00:00,  7.52it/s, total_it=6, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:01<00:00,  7.69it/s, total_it=7, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:01<00:00,  7.79it/s, total_it=8, loss=0.33]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:01<00:00,  8.17it/s, total_it=9, loss=0.33]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.31775816154059344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs biker\n",
            "weights:tensor([0.3355, 0.1678], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.9164290361641162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.40it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.13it/s, total_it=1, loss=0.86]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.13it/s, total_it=2, loss=0.90]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7490443058367421\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs car\n",
            "weights:tensor([0.1654, 0.1678], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:1.0349839874096345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.43it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  4.98it/s, total_it=1, loss=1.05]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  5.68it/s, total_it=2, loss=0.93]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.8106053814958466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs car\n",
            "weights:tensor([0.1654, 0.3355], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.9333454877578482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.39it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.21it/s, total_it=1, loss=0.96]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.22it/s, total_it=2, loss=0.90]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7779736609791642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
            "\n",
            "                                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs ped\n",
            "weights:tensor([0.1678, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.7711108468281892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.32it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.15it/s, total_it=1, loss=0.79]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.15it/s, total_it=2, loss=0.77]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7211755880097506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs biker\n",
            "weights:tensor([0.3355, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.6987449922756981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.27it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.05it/s, total_it=1, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.03it/s, total_it=2, loss=0.69]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.6943180858702235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs car\n",
            "weights:tensor([0.1654, 0.3312], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.7067280909139873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:00<00:00,  3.46it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:00<00:00,  5.24it/s, total_it=1, loss=0.71]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:00<00:00,  6.20it/s, total_it=2, loss=0.71]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.6993475291335725\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# instance_seg"
      ],
      "metadata": {
        "id": "xRGjJqKXsNpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys \n",
        "import os \n",
        "import os.path as osp \n",
        "import numpy as np \n",
        "import json \n",
        "from sklearn.cluster import DBSCAN\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt \n",
        "from scipy.spatial import distance\n",
        "from copy import deepcopy\n",
        "from cylinder_cluster import cylinder_cluster\n",
        "\"\"\" \n",
        "Do some work on object level\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def cal_precision_recall_single(labels_true, labels_pred, instance_id_true, instance_id_pred, class_label=0, targets_for_debug=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ------------- \n",
        "    This function deals with single frame precision and recall calculation\n",
        "    labels_true: 1-d array\n",
        "        The true label of the targets of one single frame\n",
        "    labels_pred: 1-d array\n",
        "        The predicted label of the targets of one single frame\n",
        "    instance_id_true: 1-d array\n",
        "        The instance ID of the targets of one single frame from SSD bounding box\n",
        "    instance_id_pred: 1-d array\n",
        "        The instance ID of the post-clustering output\n",
        "    class_label: an int\n",
        "        The label of the class to calculation. 0 for background, 1 for pedestrian, 2 for cyclist, 3 for car\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    num_TP: an int\n",
        "        The number of true positives of the class_label\n",
        "    \"\"\"\n",
        "\n",
        "    num_TP_in_pred = 0\n",
        "    num_TP_in_true = 0\n",
        "    num_instances_pred = 0\n",
        "    num_instances_true = 0\n",
        "    mask_class_label_pred = labels_pred == class_label\n",
        "    mask_class_label_true = labels_true == class_label\n",
        "    num_instances_pred = np.unique(instance_id_pred[labels_pred==class_label]).shape[0]\n",
        "    num_instances_true = np.unique(instance_id_true[labels_true==class_label]).shape[0]\n",
        "    intersection_sum = 0\n",
        "    union_sum = 0\n",
        "\n",
        "    if instance_id_pred.shape[0] >0:\n",
        "        instance_id_pred_max = instance_id_pred.max()\n",
        "    else:\n",
        "        instance_id_pred_max = 0\n",
        "    if instance_id_true.shape[0] >0:\n",
        "        instance_id_true_max = instance_id_true.max()\n",
        "    else:\n",
        "        instance_id_true_max = 0\n",
        "\n",
        "    intersection_sum = np.sum(np.logical_and(mask_class_label_pred, mask_class_label_true))\n",
        "    union_sum = np.sum(np.logical_or(mask_class_label_pred, mask_class_label_true))\n",
        "    num_TP_single_target = 0\n",
        "    # calculate num_TP_in_pred (for precision)\n",
        "    for i in np.arange(instance_id_pred_max + 1):\n",
        "        indx_pred = np.logical_and(instance_id_pred == i, mask_class_label_pred)\n",
        "        for j in np.arange(instance_id_true_max + 1):\n",
        "            indx_true = np.logical_and(instance_id_true == j, mask_class_label_true)\n",
        "            intersection = np.sum(np.logical_and(indx_pred, indx_true))\n",
        "            union = np.sum(np.logical_or(indx_pred, indx_true))\n",
        "            if intersection/union >= 0.5:\n",
        "                num_TP_in_pred += 1\n",
        "                break\n",
        "\n",
        "    # calculate num_TP_in_true (for recall)\n",
        "    for i in np.arange(instance_id_true_max + 1):\n",
        "        indx_true = np.logical_and(instance_id_true == i, mask_class_label_true)\n",
        "        for j in np.arange(instance_id_pred_max + 1):\n",
        "            indx_pred = np.logical_and(instance_id_pred == j, mask_class_label_pred)\n",
        "            intersection = np.sum(np.logical_and(indx_pred, indx_true))\n",
        "            union = np.sum(np.logical_or(indx_pred, indx_true))\n",
        "            num_true = np.sum(indx_true)\n",
        "            num_pred = np.sum(indx_pred)\n",
        "\n",
        "            if intersection/union >= 0.5:\n",
        "                num_TP_in_true += 1\n",
        "                if np.sum(indx_true) == 1:\n",
        "                    num_TP_single_target +=1\n",
        "                break\n",
        "\n",
        "                \n",
        "\n",
        "    return num_TP_in_pred, num_TP_in_true, num_instances_pred, num_instances_true, intersection_sum, union_sum, num_TP_single_target\n",
        "    \n",
        "def cal_precision_recall_all(labels_true, labels_pred, instance_id_true, instance_id_pred, frame_id, num_class = 4, targets_for_debug=None):\n",
        "    \"\"\" \n",
        "    Parameters\n",
        "    -------------\n",
        "    labels_true: 1-d array\n",
        "        The true labels of the targets of all the frames\n",
        "    labels_pared: 1-d array\n",
        "        The predicted labels of the targets of all the frames \n",
        "    instance_id_true:\n",
        "        The instance ID of the targets of all frames from SSD bounding box\n",
        "    instance_id_pred:\n",
        "        The instance ID of the post-clustering output\n",
        "    frame_id: 1-d array\n",
        "        The frame_id of each target. The size of frame_id should be equal to labels_true and labels_pred \n",
        "\n",
        "    Returns\n",
        "    -------------\n",
        "    precision: 1-d array \n",
        "        the precision of each class\n",
        "    recall: 1-d array\n",
        "        the recall of each class\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    num_TP_in_pred = np.zeros(num_class)\n",
        "    num_TP_in_true = np.zeros(num_class)\n",
        "    intersection_sum = np.zeros(num_class)\n",
        "    union_sum = np.zeros(num_class)\n",
        "    num_instance_pred = np.zeros(num_class)\n",
        "    num_instance_true = np.zeros(num_class)\n",
        "    precision = np.zeros(num_class)\n",
        "    recall = np.zeros(num_class)\n",
        "    num_TP_single_target_total = 0\n",
        "    pbar = tqdm.tqdm(total = frame_id.max()+1, desc='calculate precision and recall')\n",
        "    for i in np.arange(0, frame_id.max()+1):\n",
        "        pbar.update()\n",
        "        # if i<100:\n",
        "        #     continue\n",
        "        labels_true_single = labels_true[frame_id==i]\n",
        "        labels_pred_single = labels_pred[frame_id==i]\n",
        "        instance_id_pred_single = instance_id_pred[frame_id==i]\n",
        "        instance_id_true_single = instance_id_true[frame_id==i]\n",
        "        for j in np.arange(1, num_class):\n",
        "            num_TP_in_pred_single, num_TP_in_true_single, num_instance_pred_single, num_instance_true_single, intersection_single, union_single, num_TP_single_target = cal_precision_recall_single(labels_true_single, \n",
        "                                                                                                            labels_pred_single, \n",
        "                                                                                                            instance_id_true_single, \n",
        "                                                                                                            instance_id_pred_single, \n",
        "                                                                                                            class_label=j)\n",
        "                                                                                                            # targets_for_debug=targets_for_debug[frame_id==i,:])\n",
        "            # print(\"num_TP_in_pred_single\",num_TP_in_pred_single)\n",
        "            # print(\"num_TP_in_true_single\", num_TP_in_true_single)\n",
        "\n",
        "            num_TP_in_pred[j] += num_TP_in_pred_single\n",
        "            num_TP_in_true[j] += num_TP_in_true_single\n",
        "            num_instance_pred[j] += num_instance_pred_single\n",
        "            num_instance_true[j] += num_instance_true_single\n",
        "            intersection_sum[j] += intersection_single\n",
        "            union_sum[j] += union_single\n",
        "            num_TP_single_target_total += num_TP_single_target\n",
        "    precision = num_TP_in_pred / num_instance_pred\n",
        "    recall    = num_TP_in_true / num_instance_true\n",
        "\n",
        "    return precision, recall, intersection_sum/union_sum, num_TP_single_target_total\n",
        "\n",
        "eps_xy_list = [None, 0.5, 2 , 4]\n",
        "eps_v_list = [None, 2, 1.6, 1]\n",
        "min_targets_list = [None, 1, 2, 3]\n",
        "def post_clustering(targets_xyv, labels_pred, frame_id, DBSCAN_eps = 1.1, DBSCAN_min_samples=1, algorithm=1, target_scores = None, filter_objects = False):\n",
        "    \"\"\"  \n",
        "    Parameters:\n",
        "    ------------------\n",
        "    targets_xyv: 2-d array\n",
        "        The x, y coordinates and velocity of targets\n",
        "    labels_pred: 1-d array\n",
        "        The predicted labels of the targets of all the frames \n",
        "    frame_id: 1-d array \n",
        "        The frame_id of each target. The size of frame_id should be equal to labels_pred and targets_rav\n",
        "    \"\"\"\n",
        "    color_LUT = np.array(['c','g','r','b'])\n",
        "    min_scores_list = []\n",
        "    post_clst_id = -1 * np.ones(targets_xyv.shape[0])\n",
        "    pbar = tqdm.tqdm(total = frame_id.max()+1, desc='post_clustering')\n",
        "    t = 0\n",
        "    debug_mode = True\n",
        "    filter_objects = True\n",
        "    for i in np.arange(0, frame_id.max() + 1):\n",
        "        pbar.update()\n",
        "        if debug_mode and i < 3803:\n",
        "            continue\n",
        "        targets_xyv_single = targets_xyv[frame_id==i, :]\n",
        "        labels_pred_single = labels_pred[frame_id==i]\n",
        "        targets_xyv_ped = targets_xyv_single[labels_pred_single==1, :]\n",
        "        targets_xyv_biker = targets_xyv_single[labels_pred_single==2, :]\n",
        "        targets_xyv_car = targets_xyv_single[labels_pred_single==3, :]\n",
        "        targets_scores_single = target_scores[frame_id == i, :] / np.reshape(np.linalg.norm(target_scores[frame_id == i, :] , ord=2, axis=1), (-1, 1))\n",
        "        # first time\n",
        "        if targets_xyv_ped.shape[0] > 0:\n",
        "            post_clst_id_ped = cylinder_cluster(targets_xyv_ped[:, :3], eps_xy = eps_xy_list[1], eps_v = eps_v_list[1], min_targets=min_targets_list[1])\n",
        "            max_clst_id_ped = post_clst_id_ped.max()\n",
        "        else:\n",
        "            post_clst_id_ped = np.array([])\n",
        "            max_clst_id_ped = -1\n",
        "        if targets_xyv_biker.shape[0] > 1:\n",
        "            post_clst_id_biker = cylinder_cluster(targets_xyv_biker[:, :3], eps_xy = eps_xy_list[2], eps_v = eps_v_list[2], min_targets=min_targets_list[2])\n",
        "                \n",
        "            max_clst_id_biker = max_clst_id_ped + 1 + post_clst_id_biker.max()\n",
        "        else:\n",
        "            post_clst_id_biker = -1 * np.ones([targets_xyv_biker.shape[0]])\n",
        "            max_clst_id_biker = max_clst_id_ped \n",
        "        if targets_xyv_car.shape[0] > 2:\n",
        "            post_clst_id_car = cylinder_cluster(targets_xyv_car[:, :3], eps_xy = eps_xy_list[3], eps_v = eps_v_list[3], min_targets=min_targets_list[3])\n",
        "            \n",
        "        else:\n",
        "            post_clst_id_car = -1 * np.ones([targets_xyv_car.shape[0]])\n",
        "\n",
        "        post_clst_id_biker[post_clst_id_biker>-1] = post_clst_id_biker[post_clst_id_biker>-1] + max_clst_id_ped+1\n",
        "        post_clst_id_car[post_clst_id_car>-1] = post_clst_id_car[post_clst_id_car>-1] + max_clst_id_biker + 1\n",
        "\n",
        "        post_clst_id_single = -1 * np.ones(np.sum(frame_id == i))\n",
        "        post_clst_id_single[labels_pred_single==1] = post_clst_id_ped \n",
        "        post_clst_id_single[labels_pred_single==2] = post_clst_id_biker\n",
        "        post_clst_id_single[labels_pred_single==3] = post_clst_id_car\n",
        "        labels_pred_single[post_clst_id_single==-1] = 0\n",
        "        if debug_mode:\n",
        "            plt.figure(figsize = (16, 16))\n",
        "            plt.title(\"Frame:{}\".format(i))\n",
        "            ax1 = plt.subplot(221)\n",
        "            ax1.set_title(\"cluster before refinement\")\n",
        "            sc1 = ax1.scatter(targets_xyv_single[:, 1], targets_xyv_single[:, 0], c = post_clst_id_single, s = 10*post_clst_id_single+7)\n",
        "            plt.colorbar(sc1, ax=ax1)\n",
        "            ax1.set_xlim([-25, 25])\n",
        "            ax1.set_ylim([0, 40])\n",
        "            ax2 = plt.subplot(222)\n",
        "            ax2.set_title(\"labels before refinement\")\n",
        "            ax2.scatter(targets_xyv_single[:, 1], targets_xyv_single[:, 0], c = color_LUT[labels_pred_single])\n",
        "            ax2.set_xlim([-25, 25])\n",
        "            ax2.set_ylim([0, 40])\n",
        "\n",
        "        space_threshold = 1\n",
        "        speed_threshold = [0, 3, 2, 1.2]\n",
        "        score_threshold = 0.6\n",
        "\n",
        "        if post_clst_id_single.shape[0] > 0 and filter_objects:\n",
        "            min_dist_mat = 10 * np.ones([int(post_clst_id_single.max() + 1), int(post_clst_id_single.max() + 1)])\n",
        "            min_v_diff_mat = 10 * np.ones([int(post_clst_id_single.max() + 1), int(post_clst_id_single.max() + 1)])\n",
        "            object_label_list = 5 * np.ones(int(post_clst_id_single.max() + 1))\n",
        "            for k in np.arange(int(post_clst_id_single.max() + 1)):\n",
        "                for l in np.arange(int(post_clst_id_single.max() + 1)):\n",
        "                    if k!=l and np.sum(post_clst_id_single == k) == 0 or np.sum(post_clst_id_single == l) == 0:\n",
        "                        continue\n",
        "                    object_label_list[k] = labels_pred_single[post_clst_id_single==k][0]\n",
        "                    min_dist_pair = distance.cdist(targets_xyv_single[post_clst_id_single == k, :2], targets_xyv_single[post_clst_id_single == l, :2]).min()\n",
        "                    min_dist_mat[k, l] = min_dist_pair \n",
        "                    min_v_diff_pair = distance.cdist(np.reshape(targets_xyv_single[post_clst_id_single == k, 2], (-1, 1)), np.reshape(targets_xyv_single[post_clst_id_single == l, 2], (-1, 1))).min()\n",
        "                    min_v_diff_mat[k, l] = min_v_diff_pair\n",
        "                    min_score_diff_pair = distance.cdist(targets_scores_single[post_clst_id_single == k, :], targets_scores_single[post_clst_id_single == l, :]).min()\n",
        "                    if min_dist_pair < space_threshold:\n",
        "                        label1 = labels_pred_single[post_clst_id_single == k][0] \n",
        "                        label2 = labels_pred_single[post_clst_id_single == l][0]\n",
        "                        if (label1 == 2 and label2 == 3) or (label1 == 3 and label2 == 2):\n",
        "                            min_scores_list.append(min_score_diff_pair)\n",
        "                            # print(min_scores_list)\n",
        "                            if min_v_diff_pair < speed_threshold[3]:\n",
        "                                num_targets1 = np.sum(post_clst_id_single == k)\n",
        "                                num_targets2 = np.sum(post_clst_id_single == l)\n",
        "                                if num_targets1 > num_targets2:\n",
        "                                    label_refine = label1\n",
        "                                else:\n",
        "                                    label_refine = label2 \n",
        "                                if label_refine == 3 and min_score_diff_pair < score_threshold:\n",
        "                                    post_clst_id_single[post_clst_id_single == k] = l\n",
        "                                    labels_pred_single[post_clst_id_single == k] = label_refine\n",
        "                                    labels_pred_single[post_clst_id_single == l] = label_refine\n",
        "                        elif (label1 == 1 and label2 == 3) or (label1 == 3 and label2 == 1):\n",
        "                            if min_v_diff_pair < speed_threshold[3]:\n",
        "                                num_targets1 = np.sum(post_clst_id_single == k)\n",
        "                                num_targets2 = np.sum(post_clst_id_single == l)\n",
        "                                if num_targets1 > num_targets2:\n",
        "                                    label_refine = label1\n",
        "                                else:\n",
        "                                    label_refine = label2 \n",
        "                                if label_refine == 3  and min_score_diff_pair < score_threshold:\n",
        "                                    post_clst_id_single[post_clst_id_single == k] = l\n",
        "                                    labels_pred_single[post_clst_id_single == k] = label_refine\n",
        "                                    labels_pred_single[post_clst_id_single == l] = label_refine\n",
        "                        elif (label1 == 1 and label2 == 2) or (label1 == 2 and label2 == 1):\n",
        "                            if min_v_diff_pair < speed_threshold[2]:\n",
        "                                num_targets1 = np.sum(post_clst_id_single == k)\n",
        "                                num_targets2 = np.sum(post_clst_id_single == l)\n",
        "                                if num_targets1 > num_targets2:\n",
        "                                    label_refine = label1\n",
        "                                else:\n",
        "                                    label_refine = label2 \n",
        "                                if label_refine == 2 and min_score_diff_pair < score_threshold:\n",
        "                                    post_clst_id_single[post_clst_id_single == k] = l\n",
        "                                    labels_pred_single[post_clst_id_single == k] = label_refine\n",
        "                                    labels_pred_single[post_clst_id_single == l] = label_refine\n",
        "            num_obj_around = np.sum(np.logical_and(min_dist_mat < space_threshold, min_v_diff_mat < speed_threshold[3]), axis = 1)\n",
        "            id_list_of_cars_surrounded_by_a_lot_of_bikers = np.nonzero(np.logical_and(num_obj_around>2, object_label_list == 3))\n",
        "            for id_of_cars_surrounded_by_a_lot_of_bikers in id_list_of_cars_surrounded_by_a_lot_of_bikers:\n",
        "                labels_pred_single[post_clst_id_single == id_of_cars_surrounded_by_a_lot_of_bikers] = 2\n",
        "\n",
        "        # second time                     \n",
        "        targets_xyv_ped = targets_xyv_single[labels_pred_single==1, :]\n",
        "        targets_xyv_biker = targets_xyv_single[labels_pred_single==2, :]\n",
        "        targets_xyv_car = targets_xyv_single[labels_pred_single==3, :]\n",
        "        if targets_xyv_ped.shape[0] > 0:\n",
        "            post_clst_id_ped = cylinder_cluster(targets_xyv_ped[:, :3], eps_xy = eps_xy_list[1], eps_v = eps_v_list[1], min_targets=min_targets_list[1])\n",
        "            max_clst_id_ped = post_clst_id_ped.max()\n",
        "        else:\n",
        "            post_clst_id_ped = -1 * np.ones([targets_xyv_ped.shape[0]])\n",
        "            max_clst_id_ped = -1\n",
        "        if targets_xyv_biker.shape[0] > 1:\n",
        "            post_clst_id_biker = cylinder_cluster(targets_xyv_biker[:, :3], eps_xy = eps_xy_list[2], eps_v = eps_v_list[2], min_targets=min_targets_list[2] )\n",
        "            max_clst_id_biker = max_clst_id_ped + 1 + post_clst_id_biker.max()\n",
        "        else:\n",
        "            post_clst_id_biker = -1 * np.ones([targets_xyv_biker.shape[0]])\n",
        "            max_clst_id_biker = max_clst_id_ped \n",
        "        if targets_xyv_car.shape[0] > 2:\n",
        "            post_clst_id_car = cylinder_cluster(targets_xyv_car[:, :3], eps_xy = eps_xy_list[3], eps_v = eps_v_list[3], min_targets=min_targets_list[3])\n",
        "            \n",
        "        else:\n",
        "            post_clst_id_car = -1 * np.ones([targets_xyv_car.shape[0]])\n",
        "\n",
        "        post_clst_id_biker[post_clst_id_biker>-1] = post_clst_id_biker[post_clst_id_biker>-1] + max_clst_id_ped+1\n",
        "        post_clst_id_car[post_clst_id_car>-1] = post_clst_id_car[post_clst_id_car>-1] + max_clst_id_biker + 1\n",
        "\n",
        "        post_clst_id_single = -1 * np.ones(np.sum(frame_id == i))\n",
        "        post_clst_id_single[labels_pred_single==1] = post_clst_id_ped \n",
        "        post_clst_id_single[labels_pred_single==2] = post_clst_id_biker\n",
        "        post_clst_id_single[labels_pred_single==3] = post_clst_id_car\n",
        "        labels_pred_single[post_clst_id_single==-1] = 0\n",
        "\n",
        "        post_clst_id[frame_id==i] = post_clst_id_single\n",
        "        labels_pred[frame_id==i] = labels_pred_single\n",
        "\n",
        "    min_scores_list = np.array(min_scores_list)\n",
        "    return post_clst_id, labels_pred \n",
        "def cal_f1(precision, recall):\n",
        "\n",
        "    return 2*precision*recall/(precision+recall)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Test the post_clustering->precision & recall pipeline \"\"\"\n",
        "    BASE_DIR = '/content/RTCnet'\n",
        "    result_DIR = osp.join(BASE_DIR, 'results', 'RTCtrain_info')\n",
        "    config_list = os.listdir(result_DIR)\n",
        "    config_list.sort()\n",
        "    RTCnet_result_info_path = osp.join(result_DIR, config_list[-1])\n",
        "    speed_threshold = 0.3\n",
        "    speed_threshold_to_change_label = 0\n",
        "    show_RTC_result = True\n",
        "    with open(RTCnet_result_info_path, 'r') as f:\n",
        "        RTCnet_result_info = json.load(f)\n",
        "\n",
        "\n",
        "    if show_RTC_result:\n",
        "        result_path = RTCnet_result_info[\"result_folder\"]\n",
        "\n",
        "        data_path = RTCnet_result_info[\"data_path\"]\n",
        "       \n",
        "        labels_pred = np.load(osp.join(result_path, \"pred_labels_test.npy\"))\n",
        "        labels_true = np.load(osp.join(result_path, \"true_labels_test.npy\"))\n",
        "        target_scores = np.load(osp.join(result_path, \"final_score.npy\"))\n",
        "        frame_id = np.load(osp.join(data_path, \"test\", \"frame_id.npy\"))\n",
        "        instance_id_true = np.load(osp.join(data_path, \"instance_id_test.npy\"))\n",
        "        targets_rav = np.load(osp.join(data_path, \"test\", \"features.npy\"))[:, :3]\n",
        "\n",
        "\n",
        "        targets_xyv = np.zeros(targets_rav.shape)\n",
        "        targets_xyv[:,0] = targets_rav[:,0]*np.cos(targets_rav[:,1])\n",
        "        targets_xyv[:,1] = targets_rav[:,0]*np.sin(targets_rav[:,1])\n",
        "        targets_xyv[:,2] = targets_rav[:,2]\n",
        "        targets_v = np.abs(targets_rav[:, 2])\n",
        "        labels_pred[targets_v < speed_threshold_to_change_label] = 0\n",
        "        labels_pred = labels_pred[targets_v > speed_threshold]\n",
        "        labels_true = labels_true[targets_v > speed_threshold]\n",
        "        target_scores = target_scores[targets_v > speed_threshold]\n",
        "        frame_id = frame_id[targets_v > speed_threshold]\n",
        "        targets_rav = targets_rav[targets_v>speed_threshold, :]\n",
        "        targets_xyv = targets_xyv[targets_v > speed_threshold, :]\n",
        "        \n",
        "        instance_id_true = instance_id_true[targets_v > speed_threshold]\n",
        "        instance_id_pred, labels_pred = post_clustering(targets_xyv, labels_pred, frame_id, target_scores = target_scores)\n",
        "        precision_RTC, recall_RTC, IoU_RTC, num_TP_single_target_total = cal_precision_recall_all(labels_true, labels_pred, instance_id_true, instance_id_pred, frame_id, targets_for_debug=targets_xyv)\n",
        " \n",
        "        print(\"precision_RTC\", precision_RTC, \"recall_RTC\", recall_RTC)\n",
        "        np.savetxt(osp.join(result_path, \"precision_RTC.csv\"), precision_RTC, delimiter=\",\")\n",
        "        np.savetxt(osp.join(result_path, \"recall_RTC.csv\"), recall_RTC, delimiter=\",\")\n",
        "        np.savetxt(osp.join(result_path, \"F1score.csv\"), cal_f1(precision_RTC, recall_RTC), delimiter=\",\")\n",
        "        np.savetxt(osp.join(result_path, \"IoU_RTC.csv\"), IoU_RTC, delimiter=\",\")\n",
        "        np.save(osp.join(result_path, \"pred_labels_refine.npy\"), labels_pred)\n",
        "        print(\"number of TP object with 1 target:\", num_TP_single_target_total)\n",
        "        print(\"f1 RTC: \", cal_f1(precision_RTC, recall_RTC))\n",
        "        print(\"IoU RTC:\", IoU_RTC)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "CLeQ3oSFbuEP",
        "outputId": "660c6ebe-d60e-4c57-d4c3-1e1165d76569"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-aa1ebda98a03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRTCnet_result_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mlabels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pred_labels_test.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0mlabels_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true_labels_test.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mtarget_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_score.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/RTCnet/RTCnet/../results/RTCresults/1651631502/pred_labels_test.npy'"
          ]
        }
      ]
    }
  ]
}