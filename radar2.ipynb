{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiheddachraoui/P2M/blob/main/radar2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUS9syJLaFo7",
        "outputId": "82fc45d2-0508-46f3-a0e1-d9d017b5f0df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'radar_cnn' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "!git clone https://github.com/jiheddachraoui/radar_cnn.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kdCEPfjraddm"
      },
      "outputs": [],
      "source": [
        "import sys    \n",
        "path = '/content/radar_cnn'\n",
        "\n",
        "sys.path.append(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTF5ijy2llqw"
      },
      "source": [
        "# link with Git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBspCdtBlkzI"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"jiheddachraoui@outlook.fr\"\n",
        "!git config --global user.name \"jiheddachraoui\"\n",
        "!git config --global user.password \"xxxxx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQPHRyIVnipX",
        "outputId": "dbb87f3b-efe0-47f6-b628-d70f8756cee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'radar_cnn'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 84 (delta 37), reused 29 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (84/84), done.\n"
          ]
        }
      ],
      "source": [
        "token = 'your_token'\n",
        "username = 'jiheddachraoui'\n",
        "repo = 'radar_cnn'\n",
        "!git clone https://{token}@github.com/{username}/{repo}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpxfInUhw9YO",
        "outputId": "1937044f-d973-43cd-940c-3d627c7ce95f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQdJVjSnx2-d"
      },
      "source": [
        "**push**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKP-iOy8upQn",
        "outputId": "7b618011-a112-499a-a602-99ee57997b93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/radar_cnn\n"
          ]
        }
      ],
      "source": [
        "%cd {repo}\n",
        "!git add --all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUlBQEoFuwNP",
        "outputId": "569d1861-9670-484e-b5cd-12877dce81dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "\u001b[0m\u001b[01;34mbash\u001b[0m/                \u001b[01;34mdataset\u001b[0m/  gen_dataset.py    LICENSE    \u001b[01;34mRTCnet\u001b[0m/\n",
            "cylinder_cluster.py  \u001b[01;34mfigures\u001b[0m/  \u001b[01;32minstance_seg.py\u001b[0m*  README.md\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"first c\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5FF79rUyGAR",
        "outputId": "140ccb63-985a-44f8-b35a-9017a3a01a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Everything up-to-date\n"
          ]
        }
      ],
      "source": [
        "!git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m0Zu7gEyIrg"
      },
      "source": [
        "**pull**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrH5iO2iyLp0",
        "outputId": "31de4b61-4314-4cca-9f8d-49a96a7bfaa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Mk31W8zsVa"
      },
      "source": [
        "# Generate DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dWjuoHDuakz3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as osp \n",
        "import json\n",
        "\n",
        "import numpy as np \n",
        "\n",
        "def create_path():\n",
        "    \"\"\"\n",
        "    Create the path for the dataset.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    None\n",
        "    \n",
        "    Returnes\n",
        "    --------\n",
        "    dataset_DIR: string\n",
        "                 The path to the dataset folder\n",
        "    data_DIR: string\n",
        "              The path to the data parent folder\n",
        "    \"\"\"\n",
        "    #BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
        "    #BASE_DIR = \"/content/RTCnet\"\n",
        "    BASE_DIR = \"/content/radar_cnn\"\n",
        "    dataset_DIR = osp.join(BASE_DIR, 'dataset')\n",
        "    data_DIR = osp.join(dataset_DIR, 'data')\n",
        "    train_DIR = osp.join(data_DIR, 'train')\n",
        "    test_DIR = osp.join(data_DIR, 'test')\n",
        "    val_DIR = osp.join(data_DIR, 'val')\n",
        "    if not osp.exists(train_DIR):\n",
        "        os.makedirs(train_DIR)\n",
        "    if not osp.exists(test_DIR):\n",
        "        os.makedirs(test_DIR)\n",
        "    if not osp.exists(val_DIR):\n",
        "        os.makedirs(val_DIR)\n",
        "    return dataset_DIR, data_DIR\n",
        "\n",
        "\n",
        "def generate_dataset_single(num_features, num_samples):\n",
        "    \n",
        "    \"\"\"\n",
        "    Generate each single dataset\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_features: int\n",
        "                  The number of features for each sample\n",
        "    num_samples: int\n",
        "                 The number of samples for the dataset\n",
        "    \"\"\"\n",
        "    features_x = 120 * np.random.rand(num_samples,1) \n",
        "    features_y = 100 * np.random.rand(num_samples,1) - 50\n",
        "    features_v = 30 * np.random.rand(num_samples,1) - 15\n",
        "    features_rcs = 5 * np.random.rand(num_samples,1) \n",
        "    features_low_level = 3000 * np.random.rand(num_samples, num_features - 4)\n",
        "    features = np.concatenate([features_x, features_y, features_v, features_rcs, features_low_level], axis=1)   \n",
        "    labels = np.random.randint(4,size=(num_samples)) \n",
        "    num_targets_per_frame = 20\n",
        "    num_frames = num_samples / num_targets_per_frame\n",
        "    frame_id = np.arange(num_frames + 1)\n",
        "    frame_id = np.repeat(frame_id, num_targets_per_frame)\n",
        "    frame_id = frame_id[:num_samples]\n",
        "    instance_id = labels = np.random.randint(6,size=(num_samples)) \n",
        "\n",
        "    return features, labels, frame_id, instance_id\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_DIR, data_DIR = create_path()\n",
        "    num_features = 804\n",
        "    num_samples_train = 10000\n",
        "    num_samples_val = 1000\n",
        "    num_samples_test = 4000\n",
        "    features_train, labels_train, frame_id_train, instance_id_train  = generate_dataset_single(num_features, num_samples_train)\n",
        "    features_val, labels_val, frame_id_val, instance_id_val = generate_dataset_single(num_features, num_samples_val)\n",
        "    features_test, labels_test, frame_id_test, instance_id_test = generate_dataset_single(num_features, num_samples_test)\n",
        "    \n",
        "    \"\"\"\n",
        "    The meta data is used for configuration of the project. \n",
        "    Most attributes are related to the real data generator that generates data from real files recorded by the vehicle.\n",
        "    For example, polar_coord means whether the coordinate of radar targets will be transformed to Cartesian coordinate system.\n",
        "    \"\"\"\n",
        "    meta_data = {\n",
        "\n",
        "    \"all_mirroring\": True,\n",
        "    \"attribute\": \"all data with FOV filtering 32 cropping\",\n",
        "    \"augmentation\": True,\n",
        "    \"crop_window_size\": 5,\n",
        "    \"filter_lowlevel\": True,\n",
        "    \"mirroring\": True,\n",
        "    \"nms\": True,\n",
        "    \"polar_coord\": True,\n",
        "    \"shuffle\": True,\n",
        "    \"test_only_nearby\": False,\n",
        "    \"use_manual_annotation\": True,\n",
        "    \"view_cluster\": True,\n",
        "    \"viz_input\": False,\n",
        "    \"x_lim\": 1000,\n",
        "    \"yaw_threshold\": 100\n",
        "    }\n",
        "\n",
        "    with open(osp.join(dataset_DIR, 'meta_data.json'), 'w') as fp:\n",
        "        json.dump(meta_data, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
        "\n",
        "    np.save(osp.join(data_DIR, 'train','features.npy'), features_train)\n",
        "    np.save(osp.join(data_DIR, 'train','labels.npy'), labels_train)\n",
        "    np.save(osp.join(data_DIR, 'train','frame_id.npy'), frame_id_train)\n",
        "    np.save(osp.join(data_DIR, 'val','features.npy'), features_val)\n",
        "    np.save(osp.join(data_DIR, 'val','labels.npy'), labels_val)\n",
        "    np.save(osp.join(data_DIR, 'val','frame_id.npy'), frame_id_val)\n",
        "    np.save(osp.join(data_DIR, 'test','features.npy'), features_test)\n",
        "    np.save(osp.join(data_DIR, 'test','labels.npy'), labels_test)\n",
        "    np.save(osp.join(data_DIR, 'test','frame_id.npy'), frame_id_test)\n",
        "    np.save(osp.join(data_DIR,'instance_id_test.npy'), instance_id_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zu0_8_Hr9VE"
      },
      "source": [
        "# train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9UemTrDbjVn",
        "outputId": "40429e16-6d01-4819-c0bd-a08f9ca5b796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train feature shape:(10000, 804) \n",
            "train features after removing static targets:(10000, 804)\n",
            "train features after removing neaby and far away targets:(10000, 804)\n",
            "val feature shape:(1000, 804) \n",
            "val features after removing static targets:(1000, 804)\n",
            "val features after removing neaby and far away targets:(1000, 804)\n",
            "num_others:1675, num_peds:1660, num_bikers:1642, num_car:1630\n",
            "save the file at :/content/radar_cnn/RTCnet/../results/RTCtrain_info/1651703022.json\n",
            "start training ped vs All\n",
            "weights:tensor([0.8340, 0.1660], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.691807761410452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:01<00:12,  1.34s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:02<00:08,  1.04s/it, total_it=1, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:03<00:06,  1.06it/s, total_it=2, loss=0.65]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:03<00:05,  1.11it/s, total_it=3, loss=0.61]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:04<00:04,  1.14it/s, total_it=4, loss=0.56]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:04<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:05<00:03,  1.17it/s, total_it=5, loss=0.51]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:05<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:06<00:02,  1.20it/s, total_it=6, loss=0.47]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:06<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:07<00:01,  1.21it/s, total_it=7, loss=0.42]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:07<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:07<00:00,  1.22it/s, total_it=8, loss=0.38]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:07<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:08<00:00,  1.30it/s, total_it=9, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:08<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:09<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.5302046903642232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:09<00:00,  9.32s/it]\n",
            "\n",
            "                                                                     \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs All\n",
            "weights:tensor([0.6643, 0.3357], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.5455465756652993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:01<00:11,  1.26s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:02<00:08,  1.01s/it, total_it=1, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:02<00:06,  1.06it/s, total_it=2, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:03<00:05,  1.09it/s, total_it=3, loss=0.37]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:04<00:04,  1.12it/s, total_it=4, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:04<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:05<00:03,  1.15it/s, total_it=5, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:05<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:06<00:02,  1.17it/s, total_it=6, loss=0.36]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:06<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:07<00:01,  1.19it/s, total_it=7, loss=0.36]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:07<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:07<00:00,  1.21it/s, total_it=8, loss=0.32]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:07<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:08<00:00,  1.28it/s, total_it=9, loss=0.36]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:08<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:09<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.2788923596488413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:09<00:00,  9.34s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training car vs All\n",
            "weights:tensor([0.8309, 0.1691], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.18291068336940186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:01<00:11,  1.29s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:02<00:08,  1.03s/it, total_it=1, loss=0.20]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:02<00:06,  1.06it/s, total_it=2, loss=0.18]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:03<00:05,  1.11it/s, total_it=3, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:04<00:04,  1.14it/s, total_it=4, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:04<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:05<00:03,  1.15it/s, total_it=5, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:05<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:06<00:02,  1.17it/s, total_it=6, loss=0.18]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:06<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:07<00:01,  1.19it/s, total_it=7, loss=0.17]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:07<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:07<00:00,  1.20it/s, total_it=8, loss=0.17]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:07<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:08<00:00,  1.27it/s, total_it=9, loss=0.19]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:08<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:09<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.16462800256608395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs All\n",
            "weights:tensor([0.6709, 0.3291], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.3114321943005565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "train:  10%|█         | 1/10 [00:01<00:11,  1.29s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  20%|██        | 2/10 [00:02<00:08,  1.03s/it, total_it=1, loss=0.38]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  30%|███       | 3/10 [00:02<00:06,  1.06it/s, total_it=2, loss=0.36]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train:  40%|████      | 4/10 [00:03<00:05,  1.10it/s, total_it=3, loss=0.33]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train:  50%|█████     | 5/10 [00:04<00:04,  1.12it/s, total_it=4, loss=0.37]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:04<?, ?it/s]\n",
            "train:  60%|██████    | 6/10 [00:05<00:03,  1.14it/s, total_it=5, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:05<?, ?it/s]\n",
            "train:  70%|███████   | 7/10 [00:06<00:02,  1.16it/s, total_it=6, loss=0.34]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:06<?, ?it/s]\n",
            "train:  80%|████████  | 8/10 [00:07<00:01,  1.16it/s, total_it=7, loss=0.30]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:07<?, ?it/s]\n",
            "train:  90%|█████████ | 9/10 [00:08<00:00,  1.15it/s, total_it=8, loss=0.31]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:08<?, ?it/s]\n",
            "train: 100%|██████████| 10/10 [00:08<00:00,  1.23it/s, total_it=9, loss=0.35]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:08<?, ?it/s]\n",
            "                                                                              \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:09<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.28327470472802446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "\n",
            "                                                                      \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs biker\n",
            "weights:tensor([0.3357, 0.1660], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.9315246284125781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:02<00:02,  1.02s/it, total_it=1, loss=0.85]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:02<00:00,  1.07it/s, total_it=2, loss=0.84]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train: 100%|██████████| 4/4 [00:03<00:00,  1.55it/s, total_it=3, loss=0.80]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7518563177748752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:03<00:00,  3.51s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs car\n",
            "weights:tensor([0.1691, 0.1660], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.9582491284269807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it, total_it=1, loss=0.91]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s, total_it=2, loss=0.91]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train: 100%|██████████| 4/4 [00:03<00:00,  1.57it/s, total_it=3, loss=0.86]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  4.54it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.776298019211044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs car\n",
            "weights:tensor([0.1691, 0.3357], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.8752575049176358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:01<00:03,  1.27s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:02<00:02,  1.00s/it, total_it=1, loss=0.94]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s, total_it=2, loss=0.87]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train: 100%|██████████| 4/4 [00:03<00:00,  1.58it/s, total_it=3, loss=0.83]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.759580435477011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:03<00:00,  3.45s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs ped\n",
            "weights:tensor([0.1660, 0.3291], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.744166448517168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it, total_it=1, loss=0.79]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s, total_it=2, loss=0.76]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train: 100%|██████████| 4/4 [00:03<00:00,  1.54it/s, total_it=3, loss=0.74]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  4.60it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.7108056929110445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs biker\n",
            "weights:tensor([0.3357, 0.3291], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.6919689279706371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it, total_it=1, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s, total_it=2, loss=0.69]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train: 100%|██████████| 4/4 [00:03<00:00,  1.55it/s, total_it=3, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.6915578235568375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:03<00:00,  3.51s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs car\n",
            "weights:tensor([0.1691, 0.3291], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial_validation_loss:0.7017509786454806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "train:  25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "train:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it, total_it=1, loss=0.71]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "train:  75%|███████▌  | 3/4 [00:02<00:00,  1.06it/s, total_it=2, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "train: 100%|██████████| 4/4 [00:03<00:00,  1.53it/s, total_it=3, loss=0.70]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]\n",
            "                                                                           \u001b[A\n",
            "val:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "val: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\u001b[A\n",
            "epochs:   0%|          | 0/1 [00:03<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss is:0.6974144880584349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "train:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epochs: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
            "\n",
            "                                                                    \u001b[A"
          ]
        }
      ],
      "source": [
        "# Python Library\n",
        "import os\n",
        "import os.path as osp \n",
        "import sys \n",
        "from time import time\n",
        "import json\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "# Third-party library\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import cv2\n",
        "import easydict\n",
        "# Pytorch\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Network \n",
        "from RTCnet.RTCnet import RTCnet, TargetMLP\n",
        "from RTCnet.RTCnet_utils import Trainer\n",
        "from RTCnet.TargetLoader import TargetModeDataset, ToTensor, Permutation\n",
        "\n",
        "#BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # Base directory of the RTC module\n",
        "BASE_DIR = \"/content/radar_cnn/RTCnet\"\n",
        "\n",
        "def train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class \n",
        "            ):\n",
        "    \"\"\" \n",
        "        Function to train one-over-all model\n",
        "    Args:\n",
        "        train_data (TargetModeDataset): The training dataset\n",
        "        val_data (TargetModeDataset): The validation dataset\n",
        "        train_loader (DataLoader): The dataloader for training dataset\n",
        "        val_loader (DataLoader): The dataloader for validation dataset\n",
        "        weights_all (NumpyArray): Normalized weights for classes\n",
        "        model (RTCnet): The network modules\n",
        "        optimizer (Optimizer): The optimizer in torch.optim\n",
        "        scheduler: The scheduler for training\n",
        "        eval_frequency (int): The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "        use_gpu (bool): Whether using GPU for training and validation\n",
        "        lr_decay_step (int): the number of steps for learning rate decay\n",
        "        decay_f (double): the decay factor of learning rate\n",
        "        lr_clip (double): the clip of learning rate during decay process\n",
        "        result_folder (string): the folder to save the training result\n",
        "        n_epochs (int): the number of epochs for training\n",
        "        chose_class (int): the class number for the chosen class  \n",
        "\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs All\".format(class_list[chose_class]))\n",
        "    append_str = \"{}_vs_All\".format(class_list[chose_class])\n",
        "    weights = torch.tensor(np.array([np.sum(weights_all)-weights_all[chose_class], weights_all[chose_class]]))\n",
        "    if use_gpu:\n",
        "        weights = weights.cuda()\n",
        "    print(\"weights:{}\".format(weights))\n",
        "    loss_func = nn.CrossEntropyLoss(weight = weights)\n",
        "    #################### data ##########################\n",
        "    train_data.to_ova(chose_class = chose_class)\n",
        "    val_data.to_ova(chose_class = chose_class)\n",
        "    ################### Trainer ##########################\n",
        "    trainer = Trainer(\n",
        "                model, \n",
        "                loss_func,\n",
        "                optimizer,\n",
        "                lr_scheduler = scheduler,\n",
        "                eval_frequency = eval_frequency,\n",
        "                use_gpu = use_gpu,\n",
        "                lr_decay_step=lr_decay_step,\n",
        "                lr_decay_f=decay_f,\n",
        "                lr_clip=lr_clip,\n",
        "                save_checkerpoint_to=result_folder,\n",
        "                append_str= append_str\n",
        "    )\n",
        "    trainer.train(\n",
        "            n_epochs,\n",
        "            train_loader,\n",
        "            val_loader=val_loader,\n",
        "            best_loss=1e5,\n",
        "            start_it=0\n",
        "    )\n",
        "    loss_trajectory = trainer.trace_loss\n",
        "    loss_trajectory_train = trainer.trace_loss_train\n",
        "    np.save(osp.join(result_folder, \"loss_trajectory\" + append_str), loss_trajectory)\n",
        "    np.save(osp.join(result_folder, \"train_loss_trajectory\" + append_str), loss_trajectory_train)\n",
        "\n",
        "def train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            class_positive,\n",
        "            class_negative\n",
        "            ):\n",
        "    \"\"\" \n",
        "        Function to train one-over-one model\n",
        "\n",
        "    Args:\n",
        "        train_data (TargetModeDataset): The training dataset\n",
        "        val_data (TargetModeDataset): The validation dataset\n",
        "        train_loader (DataLoader): The dataloader for training dataset\n",
        "        val_loader (DataLoader): The dataloader for validation dataset\n",
        "        weights_all (NumpyArray): Normalized weights for classes\n",
        "        model (RTCnet): The network modules\n",
        "        optimizer (Optimizer): The optimizer in torch.optim\n",
        "        scheduler: The scheduler for training\n",
        "        eval_frequency (int): The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "        use_gpu (bool): Whether using GPU for training and validation\n",
        "        lr_decay_step (int): the number of steps for learning rate decay\n",
        "        decay_f (double): the decay factor of learning rate\n",
        "        lr_clip (double): the clip of learning rate during decay process\n",
        "        result_folder (string): the folder to save the training result\n",
        "        n_epochs (int): the number of epochs for training\n",
        "        class_positive(int): the class number for the positive class\n",
        "        class_negative(int): the class number for the negative class\n",
        "\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs {}\".format(class_list[class_positive], class_list[class_negative]))\n",
        "    append_str = \"{}_vs_{}\".format(class_list[class_positive], class_list[class_negative])\n",
        "    weights = torch.tensor(np.array([weights_all[class_negative], weights_all[class_positive]]))\n",
        "    if use_gpu:\n",
        "        weights = weights.cuda()\n",
        "    print(\"weights:{}\".format(weights))\n",
        "    loss_func = nn.CrossEntropyLoss(weight = weights)\n",
        "    #################### data ##########################\n",
        "    train_data.to_ovo(class_positive, class_negative)\n",
        "    val_data.to_ovo(class_positive, class_negative)\n",
        "    ################### Trainer ##########################\n",
        "    trainer = Trainer(\n",
        "                model, \n",
        "                loss_func,\n",
        "                optimizer,\n",
        "                lr_scheduler = scheduler,\n",
        "                eval_frequency = eval_frequency,\n",
        "                use_gpu = use_gpu,\n",
        "                lr_decay_step=lr_decay_step,\n",
        "                lr_decay_f=decay_f,\n",
        "                lr_clip=lr_clip,\n",
        "                save_checkerpoint_to=result_folder,\n",
        "                append_str= append_str\n",
        "    )\n",
        "    trainer.train(\n",
        "            n_epochs,\n",
        "            train_loader,\n",
        "            val_loader=val_loader,\n",
        "            best_loss=1e5,\n",
        "            start_it=0\n",
        "    )\n",
        "    loss_trajectory = trainer.trace_loss\n",
        "    np.save(osp.join(result_folder, \"loss_trajectory\" + append_str), loss_trajectory)\n",
        "    loss_trajectory_train = trainer.trace_loss_train\n",
        "    np.save(osp.join(result_folder, \"train_loss_trajectory\" + append_str), loss_trajectory_train)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    '''parser.add_argument(\"--data\", type=str, default = osp.join(BASE_DIR, osp.pardir, 'dataset','data'), help = \"The data path that contains train, val and test folder, this is generated by gen_input_data_and_baseline.py\" )\n",
        "    parser.add_argument(\"--batch_size\", type=int, default = 1024, help = \"batch size for training\" ) \n",
        "    parser.add_argument(\"--rm_speed\", type=bool, default = False, help = \"Whether to remove the speed during training\" )\n",
        "    parser.add_argument(\"--rm_rcs\", type=bool, default = False, help = \"Whether to remove the RCS value during training\" ) \n",
        "    parser.add_argument(\"--n_epochs\", type=int, default = 1, help = \"number of epochs\" ) '''\n",
        "    #args            = parser.parse_args()\n",
        "    #args = parser.parse_args(argv[1:])\n",
        "    args = easydict.EasyDict({\n",
        "        \"data\": '/content/radar_cnn/dataset/data',\n",
        "        \"batch_size\": 1024,\n",
        "        \"rm_speed\": False,\n",
        "        \"rm_rcs\": False,\n",
        "        \"n_epochs\": 1\n",
        "    \n",
        "})\n",
        "    data_path       = args.data\n",
        "    rm_speed        = args.rm_speed\n",
        "    rm_rcs          = args.rm_rcs \n",
        "    n_epochs        = args.n_epochs\n",
        "    model_version   = 4\n",
        "    chosen_feature_type = 'high' if model_version == 3 else 'low' \n",
        "\n",
        "    # Get the meta information of the dataset\n",
        "    data_meta_dir   = osp.join(data_path, os.pardir)\n",
        "    data_meta_path  = osp.join(data_meta_dir, 'meta_data.json')\n",
        "    with open(data_meta_path) as fp:\n",
        "        data_meta = json.load(fp)\n",
        "    data_x_lim      = data_meta['x_lim']                    # The maximum limit for the longitudinal distance\n",
        "\n",
        "    # Training setup\n",
        "    lr_start        = 1e-3                                  # The leraning rate starts from lr_start, but it decays according to certain policies\n",
        "    eval_frequency  = 0                                     # The frequency used for evaluation by validation set. 0 means evaluating after each epoch\n",
        "    batch_size      = args.batch_size                       # The batch size for stochastic gradient descent\n",
        "    lr_decay_step   = 2000                                  # The step after which the learning rate is decayed. But if the optimizer is used, this is useless.\n",
        "    decay_f         = 0.9                                   # The decay factor for the learning rate. After each lr_decay_step iterations, the learning rate is equal to decay_f * last leraning rate\n",
        "    lr_clip         = 2e-4                                  # The clip of learning rate. The learning rate stops decaying after this value\n",
        "    use_gpu         = False                                 # Whether use gpu to train\n",
        "    dropout         = True\n",
        "    if_shuffle      = True                                  # Whether to shuffle the data when loading them. This should always be true when training\n",
        "    speed_limit     = 0                                     # The speed limit used when loading data on the fly. But if the data is already filtered when generated, this can be set as 0\n",
        "    high_dims       = 4                                     # The number of dimensions for high-level features\n",
        "    dist_near       = 0                                     # The nearest distance threshold. If using polar coordinate, this is range rather than distance\n",
        "    binary_class    = False                                 # If using binary class\n",
        "    use_weight      = True                                  # If using weights for loss function\n",
        "    weights_factor  = np.array([1/0.5, 1/1, 1/0.5, 1/1])    # The scaling factor for the weights, considering unbalanced classes\n",
        "    rm_cars         = False                                 # Whether to remove cars when training\n",
        "    input_size      = 5                                     # The input size of RTC2 window\n",
        "    rm_position     = False                                 # Whether to remove position during training\n",
        "\n",
        "    t = int(time())\n",
        "    result_parent_path = osp.join(BASE_DIR, osp.pardir, 'results')\n",
        "\n",
        "    result_folder = osp.join(result_parent_path,\"RTCresults\", str(t))\n",
        "    train_info_path = osp.join(result_parent_path, 'RTCtrain_info')\n",
        "\n",
        "    if not osp.exists(train_info_path):\n",
        "        os.makedirs(train_info_path)\n",
        "    if not osp.exists(result_folder):\n",
        "        os.makedirs(result_folder)\n",
        "\n",
        "    ################### transforms ###################\n",
        "    to_tensor = ToTensor()\n",
        "    perm_position = Permutation()\n",
        "    composed_trans = transforms.Compose([perm_position, to_tensor])\n",
        "\n",
        "    ################### data loader ###################  \n",
        "    ## Train dataset \n",
        "    train_data = TargetModeDataset(\n",
        "                data_path, composed_trans, \n",
        "                mode='train', high_dims = high_dims, \n",
        "                normalize = True, feature_type = chosen_feature_type,\n",
        "                norms_path=result_folder,\n",
        "                speed_limit=speed_limit,\n",
        "                dist_near= dist_near,\n",
        "                binary_class = binary_class,\n",
        "                dist_far=data_x_lim,\n",
        "                rm_cars=rm_cars,\n",
        "                rm_position=rm_position,\n",
        "                rm_speed = rm_speed,\n",
        "                rm_rcs=rm_rcs)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=if_shuffle, num_workers=2)\n",
        "    ## Validation dataset\n",
        "    val_data = TargetModeDataset(\n",
        "                data_path, composed_trans, \n",
        "                mode='val', high_dims=high_dims, \n",
        "                normalize=True, feature_type= chosen_feature_type,\n",
        "                norms_path=result_folder,\n",
        "                speed_limit=speed_limit,\n",
        "                dist_near= dist_near,\n",
        "                binary_class = binary_class,\n",
        "                dist_far=data_x_lim,\n",
        "                rm_cars=rm_cars,\n",
        "                rm_position=rm_position,\n",
        "                rm_speed = rm_speed,\n",
        "                rm_rcs=rm_rcs)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    train_data.save_features_labels()\n",
        "    val_data.save_features_labels()\n",
        "\n",
        "    ################### Model ################### \n",
        "\n",
        "    if model_version == 4:\n",
        "        model = RTCnet(\n",
        "                num_classes=2, \n",
        "                Doppler_dims=32, \n",
        "                high_dims = high_dims, \n",
        "                dropout= dropout,\n",
        "                input_size = input_size)\n",
        "    elif model_version == 3:\n",
        "        model = TargetMLP(\n",
        "                num_classes=2,\n",
        "                high_dims=high_dims)\n",
        "\n",
        "    model.double()\n",
        "\n",
        "\n",
        "    ################### weight initialization ################### \n",
        "    if use_weight:\n",
        "        num_others = np.sum(train_data.labels==0)\n",
        "        num_ped = np.sum(train_data.labels == 1)\n",
        "        num_bikers = np.sum(train_data.labels==2)\n",
        "        num_car = np.sum(train_data.labels==3)\n",
        "        print(\"num_others:{}, num_peds:{}, num_bikers:{}, num_car:{}\".format(num_others, num_ped, num_bikers, num_car))\n",
        "\n",
        "        weights = np.array([1/num_others,1/num_ped,1/num_bikers,1/num_car])\n",
        "    else:\n",
        "        weights = np.ones(4)\n",
        "\n",
        "    weights = np.multiply(weights_factor, weights)\n",
        "    weights_save = deepcopy(weights.tolist())\n",
        "    weights_all = np.divide(weights, np.sum(weights))\n",
        "\n",
        "    ################### Save the training information #############################\n",
        "    train_info = {\n",
        "    \"data_path\": data_path, \n",
        "    \"lr_start\": lr_start,\n",
        "    \"n_epochs\": n_epochs,\n",
        "    \"eval_frequency\": eval_frequency,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"lr_decay_step\":lr_decay_step,\n",
        "    \"decay_f\":decay_f,\n",
        "    \"lr_clip\":lr_clip,\n",
        "    \"use_gpu\":use_gpu,\n",
        "    \"t\":t,\n",
        "    \"result_folder\":result_folder,\n",
        "    \"weights\": weights_save,\n",
        "    \"weights_factor\": weights_factor.tolist(),\n",
        "    \"if_shuffle\":if_shuffle,\n",
        "    \"dropout\":dropout,\n",
        "    \"speed_limit\": speed_limit,\n",
        "    \"dist_limit\": dist_near,\n",
        "    \"binary\": binary_class,\n",
        "    \"input_size\": input_size,\n",
        "    \"model_version\": model_version,\n",
        "    \"data_x_limit\": data_x_lim,\n",
        "    \"use_weight\":use_weight,\n",
        "    \"rm_cars\": rm_cars,\n",
        "    \"rm_position\": rm_position,\n",
        "    \"rm_speed\": rm_speed,\n",
        "    \"rm_rcs\": rm_rcs\n",
        "    }\n",
        "    with open(osp.join(train_info_path, '{}.json'.format(t)), 'w') as fp:\n",
        "        print(\"save the file at :{}\".format(osp.join(train_info_path, '{}.json'.format(t))))\n",
        "        json.dump(train_info, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ################### Optimizer ########################\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr_start)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=lr_decay_step,gamma=0.1)\n",
        "    \n",
        "    ################### Ped vs ALL ########################\n",
        "    \n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 1 \n",
        "            )\n",
        "\n",
        "    ################### Biker VS ALL ######################\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 2\n",
        "    ) \n",
        "    ################### Car VS ALL ######################\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 3\n",
        "    ) \n",
        "    ################### others VS ALL ######################\n",
        "\n",
        "    train_ova(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            chose_class = 0\n",
        "    ) \n",
        "\n",
        "    ################### Ped VS biker ########################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            1,\n",
        "            2\n",
        "            )\n",
        "\n",
        "    #################### Ped VS car #########################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            1,\n",
        "            3\n",
        "            )\n",
        "    ################### Biker VS car #######################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            2,\n",
        "            3\n",
        "            )\n",
        "    #################### Others VS ped #####################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            1\n",
        "            )\n",
        "    #################### Others VS biker #####################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            2\n",
        "            )\n",
        "    ##################### Others VS car #######################\n",
        "    train_ovo(train_data, \n",
        "            val_data, \n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            weights_all, \n",
        "            model, \n",
        "            optimizer, \n",
        "            scheduler, \n",
        "            eval_frequency, \n",
        "            use_gpu, \n",
        "            lr_decay_step, \n",
        "            decay_f, \n",
        "            lr_clip, \n",
        "            result_folder,\n",
        "            n_epochs,\n",
        "            0,\n",
        "            3\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qVJDv-asHcr"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R53WHZ-mbqZW",
        "outputId": "981ec915-576e-4218-816e-2c75484da1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test feature shape:(4000, 804) \n",
            "test features after removing static targets:(4000, 804)\n",
            "test features after removing neaby and far away targets:(4000, 804)\n",
            "start training ped vs All\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs All\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training car vs All\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs All\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs biker\n",
            "weights:tensor([0.0024, 0.0006], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training ped vs car\n",
            "weights:tensor([0.0006, 0.0006], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training biker vs car\n",
            "weights:tensor([0.0006, 0.0024], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs ped\n",
            "weights:tensor([0.0006, 0.0024], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs biker\n",
            "weights:tensor([0.0024, 0.0024], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start training others vs car\n",
            "weights:tensor([0.0006, 0.0024], dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ],
      "source": [
        "\n",
        "# Python Libraries\n",
        "import os \n",
        "import sys \n",
        "import os.path as osp \n",
        "import argparse\n",
        "\n",
        "# Pytorch\n",
        "import torch \n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn \n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "import json \n",
        "import easydict\n",
        "\n",
        "# Modules from current project\n",
        "from RTCnet.TargetLoader import TargetModeDataset, ToTensor\n",
        "from RTCnet.RTCnet import RTCnet, TargetMLP\n",
        "from RTCnet.RTCnet_utils import Tester, Tester_ensemble\n",
        "\n",
        "#BASE_DIR = os.path.dirname(os.path.abspath(__file__)) # Base directory of the RTC module\n",
        "BASE_DIR ='/content/radar_cnn/RTCnet'\n",
        "result_DIR = osp.join(BASE_DIR, osp.pardir, 'results', 'RTCtrain_info')\n",
        "config_list = os.listdir(result_DIR)\n",
        "config_list.sort()\n",
        "parser = argparse.ArgumentParser()\n",
        "    \n",
        "#args            = parser.parse_args()\n",
        "#args = parser.parse_args(argv[1:])\n",
        "args = easydict.EasyDict({\n",
        "        \"config\": osp.join(result_DIR, config_list[-1]),\n",
        "        \"batch_size\": 1024,\n",
        "        \"test\": None,\n",
        "        \n",
        "    \n",
        "})\n",
        "\n",
        "\n",
        "def test_ova(\n",
        "    chose_class,\n",
        "    result_folder,\n",
        "    model,\n",
        "    test_data,\n",
        "    weights_all,\n",
        "    test_loader,\n",
        "    use_gpu\n",
        "):\n",
        "    \"\"\"\n",
        "    Test the trained model on testing dataset\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    chose_class: int\n",
        "                 The class index which is chosen for one-over-all model testing\n",
        "    result_folder: string \n",
        "                   The path in which the trained result is saved\n",
        "    model: RTCnet\n",
        "           The RTC network \n",
        "    test_data: TargetModeDataset \n",
        "               The dataset for testing\n",
        "    weights_all: Array \n",
        "                 The class weights \n",
        "    test_loader: DataLoader\n",
        "                 The dataloader for testing dataset \n",
        "    use_gpu: bool \n",
        "            flag to decide whether GPU is used for acceleration\n",
        "            \n",
        "    Returns: Array shape(n_samples, n_classes)\n",
        "            The scores for each class\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs All\".format(class_list[chose_class]))\n",
        "    append_str = \"{}_vs_All\".format(class_list[chose_class])\n",
        "    model_path      = osp.join(result_folder,'best_checkerpoint_{}.pth'.format(append_str))\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    weights = torch.tensor(np.array([np.sum(weights_all) - weights_all[1], weights_all[1]])) if not use_gpu else \\\n",
        "         torch.tensor(np.array([np.sum(weights_all) - weights_all[1], weights_all[1]])).cuda()\n",
        "    loss_func = nn.CrossEntropyLoss(weights)\n",
        "\n",
        "    tester = Tester_ensemble(\n",
        "        model,\n",
        "        loss_func = loss_func,\n",
        "        use_gpu=use_gpu\n",
        "    )\n",
        "    scores_ova = tester.test(test_loader)\n",
        "    return scores_ova\n",
        "\n",
        "def test_ovo(\n",
        "    class_positive,\n",
        "    class_negative,\n",
        "    result_folder,\n",
        "    model,\n",
        "    test_data,\n",
        "    weights_all,\n",
        "    test_loader,\n",
        "    use_gpu,\n",
        "):\n",
        "    \"\"\"\n",
        "    Test the trained model on testing dataset\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    class_positive: int\n",
        "                    The class index which is chosen for positive class (one-vs-one)\n",
        "    class_negative: int\n",
        "                    The class index which is chosen for negative class (one-vs-one)\n",
        "    result_folder: string \n",
        "                   The path in which the trained result is saved\n",
        "    model: RTCnet\n",
        "           The RTC network \n",
        "    test_data: TargetModeDataset \n",
        "               The dataset for testing\n",
        "    weights_all: Array \n",
        "                 The class weights \n",
        "    test_loader: DataLoader\n",
        "                 The dataloader for testing dataset \n",
        "    use_gpu: bool \n",
        "            flag to decide whether GPU is used for acceleration\n",
        "            \n",
        "    Returns: Array shape(n_samples, n_classes)\n",
        "            The scores for each class\n",
        "    \"\"\"\n",
        "    class_list = ['others', 'ped', 'biker', 'car']\n",
        "    print(\"start training {} vs {}\".format(class_list[class_positive], class_list[class_negative]))\n",
        "    append_str = \"{}_vs_{}\".format(class_list[class_positive], class_list[class_negative])\n",
        "    weights = torch.tensor(np.array([weights_all[class_negative], weights_all[class_positive]]))\n",
        "    if use_gpu:\n",
        "        weights = weights.cuda()\n",
        "    print(\"weights:{}\".format(weights))\n",
        "    loss_func = nn.CrossEntropyLoss(weight = weights)\n",
        "\n",
        "    model_path      = osp.join(result_folder,'best_checkerpoint_{}.pth'.format(append_str))\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    tester = Tester_ensemble(\n",
        "        model,\n",
        "        loss_func = loss_func,\n",
        "        use_gpu=use_gpu\n",
        "    )\n",
        "    scores_ovo = tester.test(test_loader)\n",
        "    return scores_ovo\n",
        "\n",
        "def main():\n",
        "    cfg_path        = args.config\n",
        "    test_data_path  = args.test\n",
        "\n",
        "    with open(cfg_path) as fp:\n",
        "        cfg = json.load(fp)\n",
        "    data_path       = cfg[\"data_path\"]\n",
        "    use_gpu         = cfg['use_gpu']\n",
        "    result_folder   = cfg['result_folder']\n",
        "    weights_all     = cfg['weights']\n",
        "    use_set         = [\"train\", \"val\", \"test\"]\n",
        "    weights_factor  = cfg[\"weights_factor\"]\n",
        "    dropout         = cfg['dropout']\n",
        "    weights_factor  = np.array(weights_factor)\n",
        "    weights_all     = np.array(weights_all)\n",
        "    binary_class    = cfg['binary']\n",
        "    input_size      = cfg['input_size']\n",
        "    rm_cars         = cfg[\"rm_cars\"]\n",
        "    speed_limit     = cfg[\"speed_limit\"]\n",
        "    dist_near       = cfg[\"dist_limit\"]\n",
        "    dist_far        = cfg[\"data_x_limit\"]\n",
        "    if \"rm_speed\" in cfg:\n",
        "        rm_speed = cfg[\"rm_speed\"]\n",
        "    else:\n",
        "        rm_speed = False\n",
        "    if \"rm_rcs\" in cfg:\n",
        "        rm_rcs = cfg[\"rm_rcs\"]\n",
        "    else:\n",
        "        rm_rcs = False\n",
        "    test_with_nearby = False\n",
        "    only_slow = False\n",
        "    only_fast = False\n",
        "    if \"rm_position\" in cfg:\n",
        "        rm_position = cfg['rm_position']\n",
        "    else:\n",
        "        rm_position = False\n",
        "    \n",
        "    use_nearby = False\n",
        "    if use_nearby or test_with_nearby:\n",
        "        dist_far = 60\n",
        "    if rm_position:\n",
        "        high_dims = 2\n",
        "        dist_far = 100\n",
        "    else:\n",
        "        high_dims = 4\n",
        "    if test_data_path is None:\n",
        "        test_data_path = data_path\n",
        "    if rm_cars:\n",
        "        num_classes = 3\n",
        "    elif binary_class:\n",
        "        num_classes = 2\n",
        "    else:\n",
        "        num_classes = 2\n",
        "    weights_all = np.multiply(weights_factor, weights_all)\n",
        "    model_version   = 4\n",
        "    chosen_feature_type = 'high' if model_version == 3 else 'low'\n",
        "    cfg['testset'] = test_data_path\n",
        "    with open(osp.join(result_folder, 'info.json'), 'w') as fp:\n",
        "        json.dump(cfg, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
        "    \n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    ################### transforms ###################\n",
        "    to_tensor = ToTensor()\n",
        "    composed_trans = transforms.Compose([to_tensor])\n",
        "\n",
        "    test_data = TargetModeDataset(\n",
        "                test_data_path, composed_trans, \n",
        "                mode='test', high_dims=high_dims, \n",
        "                normalize=True, feature_type= chosen_feature_type,\n",
        "                norms_path=result_folder,\n",
        "                speed_limit=speed_limit,\n",
        "                dist_near=dist_near,\n",
        "                binary_class=binary_class,\n",
        "                dist_far=dist_far,\n",
        "                rm_cars=rm_cars,\n",
        "                rm_position=rm_position,\n",
        "                zero_low=False,\n",
        "                only_slow=only_slow,\n",
        "                only_fast=only_fast,\n",
        "                rm_speed=rm_speed,\n",
        "                rm_rcs=rm_rcs)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "    np.save(osp.join(result_folder, \"valid_indx_test\"), test_data.indx_valid)\n",
        "\n",
        "    ################### Define model ###################\n",
        "    if model_version == 4:\n",
        "        model = RTCnet(\n",
        "                num_classes=2, \n",
        "                Doppler_dims=32, \n",
        "                high_dims = high_dims, \n",
        "                dropout= dropout,\n",
        "                input_size = input_size)\n",
        "    elif model_version == 3:\n",
        "        model = TargetMLP(\n",
        "                num_classes=2,\n",
        "                high_dims=high_dims)\n",
        "\n",
        "    ################### Ped VS ALL ###########\n",
        "    scores_Ped_vs_All = test_ova(\n",
        "                        1,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Biker_vs_All = test_ova(\n",
        "                        2,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Car_vs_All = test_ova(\n",
        "                        3,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Others_vs_All = test_ova(\n",
        "                        0,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Ped_vs_Biker = test_ovo(\n",
        "                        1,\n",
        "                        2,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Ped_vs_Car = test_ovo(\n",
        "                        1,\n",
        "                        3,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "    scores_Biker_vs_Car = test_ovo(\n",
        "                        2,\n",
        "                        3,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Others_vs_Ped = test_ovo(\n",
        "                        0,\n",
        "                        1,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Others_vs_Biker = test_ovo(\n",
        "                        0,\n",
        "                        2,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    scores_Others_vs_Car = test_ovo(\n",
        "                        0,\n",
        "                        3,\n",
        "                        result_folder,\n",
        "                        model,\n",
        "                        test_data,\n",
        "                        weights_all,\n",
        "                        test_loader,\n",
        "                        use_gpu\n",
        "                    )\n",
        "\n",
        "    \n",
        "\n",
        "    final_score1 = np.reshape(scores_Ped_vs_Biker * (scores_Ped_vs_All + scores_Biker_vs_All) \\\n",
        "                    + scores_Ped_vs_Car * (scores_Ped_vs_All + scores_Car_vs_All) \\\n",
        "                    + (1 - scores_Others_vs_Ped) * (scores_Ped_vs_All + scores_Others_vs_All), (-1, 1))\n",
        "    final_score2 = np.reshape(( 1 - scores_Ped_vs_Biker) * (scores_Biker_vs_All + scores_Ped_vs_All)\\\n",
        "                    + scores_Biker_vs_Car * (scores_Biker_vs_All + scores_Car_vs_All)\\\n",
        "                    + (1 - scores_Others_vs_Biker) * (scores_Biker_vs_All + scores_Others_vs_All), (-1, 1))\n",
        "    final_score3 = np.reshape((1 - scores_Ped_vs_Car) * (scores_Car_vs_All + scores_Ped_vs_All)\\\n",
        "                    + (1-scores_Biker_vs_Car) * (scores_Car_vs_All + scores_Biker_vs_All)\\\n",
        "                    + (1-scores_Others_vs_Car) * (scores_Car_vs_All + scores_Others_vs_All), (-1, 1))\n",
        "    final_score0 = np.reshape(scores_Others_vs_Ped * (scores_Others_vs_All + scores_Ped_vs_All)\\\n",
        "                    + scores_Others_vs_Biker * (scores_Others_vs_All + scores_Biker_vs_All)\\\n",
        "                    + scores_Others_vs_Car * (scores_Others_vs_All + scores_Car_vs_All), (-1, 1))\n",
        "    final_score = np.concatenate([final_score0, final_score1, final_score2, final_score3], axis=1)\n",
        "    pred_labels_all = np.argmax(final_score, axis=1)\n",
        "    true_labels_all = test_data.labels\n",
        "\n",
        "    np.save(osp.join(result_folder, \"final_score.npy\"), final_score)\n",
        "    np.save(osp.join(result_folder, \"true_labels_test.npy\"), true_labels_all)\n",
        "\n",
        "\n",
        "    np.save(osp.join(result_folder, \"pred_labels_{}\".format(\"test\")), pred_labels_all)\n",
        "    ################### Confusion matrix ######################\n",
        "    class_names = np.array(['Others', 'Ped.', 'Biker', 'Car'])\n",
        "    confusion_mat = confusion_matrix(true_labels_all, pred_labels_all)\n",
        "    f1_score_all = f1_score(true_labels_all, pred_labels_all, average='macro')\n",
        "    f1_score_individual = f1_score(true_labels_all, \n",
        "                            pred_labels_all, average=None)\n",
        "    with open(osp.join(result_folder, \"f1score_{}.txt\".format(\"test\")),'a') as f:\n",
        "        np.savetxt(f,np.reshape(f1_score_all, (1,-1)),fmt='%f')\n",
        "        np.savetxt(f,np.reshape(f1_score_individual, (1,-1)),fmt='%f')\n",
        "    np.savetxt(osp.join(result_folder, '{}.txt'.format(\"test\")),confusion_mat,fmt='%f')\n",
        "    np.save(osp.join('{}.npy'.format(\"test\")),confusion_mat)\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRGjJqKXsNpK"
      },
      "source": [
        "# instance_seg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLeQ3oSFbuEP",
        "outputId": "321155d5-e9fd-4699-8725-e92d491bdc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "post_clustering: 100%|██████████| 200/200.0 [00:00<00:00, 251005.63it/s]\n",
            "calculate precision and recall:   0%|          | 0/200.0 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:153: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:156: RuntimeWarning: invalid value encountered in true_divide\n",
            "calculate precision and recall: 100%|██████████| 200/200.0 [00:00<00:00, 3970.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision_RTC [nan  0.  0. nan] recall_RTC [nan  0.  0.  0.]\n",
            "number of TP object with 1 target: 0\n",
            "f1 RTC:  [nan nan nan nan]\n",
            "IoU RTC: [       nan 0.10643185 0.16538343 0.        ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:334: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        }
      ],
      "source": [
        "import sys \n",
        "import os \n",
        "import os.path as osp \n",
        "import numpy as np \n",
        "import json \n",
        "from sklearn.cluster import DBSCAN\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt \n",
        "from scipy.spatial import distance\n",
        "from copy import deepcopy\n",
        "from cylinder_cluster import cylinder_cluster\n",
        "\"\"\" \n",
        "Do some work on object level\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def cal_precision_recall_single(labels_true, labels_pred, instance_id_true, instance_id_pred, class_label=0, targets_for_debug=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ------------- \n",
        "    This function deals with single frame precision and recall calculation\n",
        "    labels_true: 1-d array\n",
        "        The true label of the targets of one single frame\n",
        "    labels_pred: 1-d array\n",
        "        The predicted label of the targets of one single frame\n",
        "    instance_id_true: 1-d array\n",
        "        The instance ID of the targets of one single frame from SSD bounding box\n",
        "    instance_id_pred: 1-d array\n",
        "        The instance ID of the post-clustering output\n",
        "    class_label: an int\n",
        "        The label of the class to calculation. 0 for background, 1 for pedestrian, 2 for cyclist, 3 for car\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    num_TP: an int\n",
        "        The number of true positives of the class_label\n",
        "    \"\"\"\n",
        "\n",
        "    num_TP_in_pred = 0\n",
        "    num_TP_in_true = 0\n",
        "    num_instances_pred = 0\n",
        "    num_instances_true = 0\n",
        "    mask_class_label_pred = labels_pred == class_label\n",
        "    mask_class_label_true = labels_true == class_label\n",
        "    num_instances_pred = np.unique(instance_id_pred[labels_pred==class_label]).shape[0]\n",
        "    num_instances_true = np.unique(instance_id_true[labels_true==class_label]).shape[0]\n",
        "    intersection_sum = 0\n",
        "    union_sum = 0\n",
        "\n",
        "    if instance_id_pred.shape[0] >0:\n",
        "        instance_id_pred_max = instance_id_pred.max()\n",
        "    else:\n",
        "        instance_id_pred_max = 0\n",
        "    if instance_id_true.shape[0] >0:\n",
        "        instance_id_true_max = instance_id_true.max()\n",
        "    else:\n",
        "        instance_id_true_max = 0\n",
        "\n",
        "    intersection_sum = np.sum(np.logical_and(mask_class_label_pred, mask_class_label_true))\n",
        "    union_sum = np.sum(np.logical_or(mask_class_label_pred, mask_class_label_true))\n",
        "    num_TP_single_target = 0\n",
        "    # calculate num_TP_in_pred (for precision)\n",
        "    for i in np.arange(instance_id_pred_max + 1):\n",
        "        indx_pred = np.logical_and(instance_id_pred == i, mask_class_label_pred)\n",
        "        for j in np.arange(instance_id_true_max + 1):\n",
        "            indx_true = np.logical_and(instance_id_true == j, mask_class_label_true)\n",
        "            intersection = np.sum(np.logical_and(indx_pred, indx_true))\n",
        "            union = np.sum(np.logical_or(indx_pred, indx_true))\n",
        "            if intersection/union >= 0.5:\n",
        "                num_TP_in_pred += 1\n",
        "                break\n",
        "\n",
        "    # calculate num_TP_in_true (for recall)\n",
        "    for i in np.arange(instance_id_true_max + 1):\n",
        "        indx_true = np.logical_and(instance_id_true == i, mask_class_label_true)\n",
        "        for j in np.arange(instance_id_pred_max + 1):\n",
        "            indx_pred = np.logical_and(instance_id_pred == j, mask_class_label_pred)\n",
        "            intersection = np.sum(np.logical_and(indx_pred, indx_true))\n",
        "            union = np.sum(np.logical_or(indx_pred, indx_true))\n",
        "            num_true = np.sum(indx_true)\n",
        "            num_pred = np.sum(indx_pred)\n",
        "\n",
        "            if intersection/union >= 0.5:\n",
        "                num_TP_in_true += 1\n",
        "                if np.sum(indx_true) == 1:\n",
        "                    num_TP_single_target +=1\n",
        "                break\n",
        "\n",
        "                \n",
        "\n",
        "    return num_TP_in_pred, num_TP_in_true, num_instances_pred, num_instances_true, intersection_sum, union_sum, num_TP_single_target\n",
        "    \n",
        "def cal_precision_recall_all(labels_true, labels_pred, instance_id_true, instance_id_pred, frame_id, num_class = 4, targets_for_debug=None):\n",
        "    \"\"\" \n",
        "    Parameters\n",
        "    -------------\n",
        "    labels_true: 1-d array\n",
        "        The true labels of the targets of all the frames\n",
        "    labels_pared: 1-d array\n",
        "        The predicted labels of the targets of all the frames \n",
        "    instance_id_true:\n",
        "        The instance ID of the targets of all frames from SSD bounding box\n",
        "    instance_id_pred:\n",
        "        The instance ID of the post-clustering output\n",
        "    frame_id: 1-d array\n",
        "        The frame_id of each target. The size of frame_id should be equal to labels_true and labels_pred \n",
        "\n",
        "    Returns\n",
        "    -------------\n",
        "    precision: 1-d array \n",
        "        the precision of each class\n",
        "    recall: 1-d array\n",
        "        the recall of each class\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    num_TP_in_pred = np.zeros(num_class)\n",
        "    num_TP_in_true = np.zeros(num_class)\n",
        "    intersection_sum = np.zeros(num_class)\n",
        "    union_sum = np.zeros(num_class)\n",
        "    num_instance_pred = np.zeros(num_class)\n",
        "    num_instance_true = np.zeros(num_class)\n",
        "    precision = np.zeros(num_class)\n",
        "    recall = np.zeros(num_class)\n",
        "    num_TP_single_target_total = 0\n",
        "    pbar = tqdm.tqdm(total = frame_id.max()+1, desc='calculate precision and recall')\n",
        "    for i in np.arange(0, frame_id.max()+1):\n",
        "        pbar.update()\n",
        "        # if i<100:\n",
        "        #     continue\n",
        "        labels_true_single = labels_true[frame_id==i]\n",
        "        labels_pred_single = labels_pred[frame_id==i]\n",
        "        instance_id_pred_single = instance_id_pred[frame_id==i]\n",
        "        instance_id_true_single = instance_id_true[frame_id==i]\n",
        "        for j in np.arange(1, num_class):\n",
        "            num_TP_in_pred_single, num_TP_in_true_single, num_instance_pred_single, num_instance_true_single, intersection_single, union_single, num_TP_single_target = cal_precision_recall_single(labels_true_single, \n",
        "                                                                                                            labels_pred_single, \n",
        "                                                                                                            instance_id_true_single, \n",
        "                                                                                                            instance_id_pred_single, \n",
        "                                                                                                            class_label=j)\n",
        "                                                                                                            # targets_for_debug=targets_for_debug[frame_id==i,:])\n",
        "            # print(\"num_TP_in_pred_single\",num_TP_in_pred_single)\n",
        "            # print(\"num_TP_in_true_single\", num_TP_in_true_single)\n",
        "\n",
        "            num_TP_in_pred[j] += num_TP_in_pred_single\n",
        "            num_TP_in_true[j] += num_TP_in_true_single\n",
        "            num_instance_pred[j] += num_instance_pred_single\n",
        "            num_instance_true[j] += num_instance_true_single\n",
        "            intersection_sum[j] += intersection_single\n",
        "            union_sum[j] += union_single\n",
        "            num_TP_single_target_total += num_TP_single_target\n",
        "    precision = num_TP_in_pred / num_instance_pred\n",
        "    recall    = num_TP_in_true / num_instance_true\n",
        "\n",
        "    return precision, recall, intersection_sum/union_sum, num_TP_single_target_total\n",
        "\n",
        "eps_xy_list = [None, 0.5, 2 , 4]\n",
        "eps_v_list = [None, 2, 1.6, 1]\n",
        "min_targets_list = [None, 1, 2, 3]\n",
        "def post_clustering(targets_xyv, labels_pred, frame_id, DBSCAN_eps = 1.1, DBSCAN_min_samples=1, algorithm=1, target_scores = None, filter_objects = False):\n",
        "    \"\"\"  \n",
        "    Parameters:\n",
        "    ------------------\n",
        "    targets_xyv: 2-d array\n",
        "        The x, y coordinates and velocity of targets\n",
        "    labels_pred: 1-d array\n",
        "        The predicted labels of the targets of all the frames \n",
        "    frame_id: 1-d array \n",
        "        The frame_id of each target. The size of frame_id should be equal to labels_pred and targets_rav\n",
        "    \"\"\"\n",
        "    color_LUT = np.array(['c','g','r','b'])\n",
        "    min_scores_list = []\n",
        "    post_clst_id = -1 * np.ones(targets_xyv.shape[0])\n",
        "    pbar = tqdm.tqdm(total = frame_id.max()+1, desc='post_clustering')\n",
        "    t = 0\n",
        "    debug_mode = True\n",
        "    filter_objects = True\n",
        "    for i in np.arange(0, frame_id.max() + 1):\n",
        "        pbar.update()\n",
        "        if debug_mode and i < 3803:\n",
        "            continue\n",
        "        targets_xyv_single = targets_xyv[frame_id==i, :]\n",
        "        labels_pred_single = labels_pred[frame_id==i]\n",
        "        targets_xyv_ped = targets_xyv_single[labels_pred_single==1, :]\n",
        "        targets_xyv_biker = targets_xyv_single[labels_pred_single==2, :]\n",
        "        targets_xyv_car = targets_xyv_single[labels_pred_single==3, :]\n",
        "        targets_scores_single = target_scores[frame_id == i, :] / np.reshape(np.linalg.norm(target_scores[frame_id == i, :] , ord=2, axis=1), (-1, 1))\n",
        "        # first time\n",
        "        if targets_xyv_ped.shape[0] > 0:\n",
        "            post_clst_id_ped = cylinder_cluster(targets_xyv_ped[:, :3], eps_xy = eps_xy_list[1], eps_v = eps_v_list[1], min_targets=min_targets_list[1])\n",
        "            max_clst_id_ped = post_clst_id_ped.max()\n",
        "        else:\n",
        "            post_clst_id_ped = np.array([])\n",
        "            max_clst_id_ped = -1\n",
        "        if targets_xyv_biker.shape[0] > 1:\n",
        "            post_clst_id_biker = cylinder_cluster(targets_xyv_biker[:, :3], eps_xy = eps_xy_list[2], eps_v = eps_v_list[2], min_targets=min_targets_list[2])\n",
        "                \n",
        "            max_clst_id_biker = max_clst_id_ped + 1 + post_clst_id_biker.max()\n",
        "        else:\n",
        "            post_clst_id_biker = -1 * np.ones([targets_xyv_biker.shape[0]])\n",
        "            max_clst_id_biker = max_clst_id_ped \n",
        "        if targets_xyv_car.shape[0] > 2:\n",
        "            post_clst_id_car = cylinder_cluster(targets_xyv_car[:, :3], eps_xy = eps_xy_list[3], eps_v = eps_v_list[3], min_targets=min_targets_list[3])\n",
        "            \n",
        "        else:\n",
        "            post_clst_id_car = -1 * np.ones([targets_xyv_car.shape[0]])\n",
        "\n",
        "        post_clst_id_biker[post_clst_id_biker>-1] = post_clst_id_biker[post_clst_id_biker>-1] + max_clst_id_ped+1\n",
        "        post_clst_id_car[post_clst_id_car>-1] = post_clst_id_car[post_clst_id_car>-1] + max_clst_id_biker + 1\n",
        "\n",
        "        post_clst_id_single = -1 * np.ones(np.sum(frame_id == i))\n",
        "        post_clst_id_single[labels_pred_single==1] = post_clst_id_ped \n",
        "        post_clst_id_single[labels_pred_single==2] = post_clst_id_biker\n",
        "        post_clst_id_single[labels_pred_single==3] = post_clst_id_car\n",
        "        labels_pred_single[post_clst_id_single==-1] = 0\n",
        "        if debug_mode:\n",
        "            plt.figure(figsize = (16, 16))\n",
        "            plt.title(\"Frame:{}\".format(i))\n",
        "            ax1 = plt.subplot(221)\n",
        "            ax1.set_title(\"cluster before refinement\")\n",
        "            sc1 = ax1.scatter(targets_xyv_single[:, 1], targets_xyv_single[:, 0], c = post_clst_id_single, s = 10*post_clst_id_single+7)\n",
        "            plt.colorbar(sc1, ax=ax1)\n",
        "            ax1.set_xlim([-25, 25])\n",
        "            ax1.set_ylim([0, 40])\n",
        "            ax2 = plt.subplot(222)\n",
        "            ax2.set_title(\"labels before refinement\")\n",
        "            ax2.scatter(targets_xyv_single[:, 1], targets_xyv_single[:, 0], c = color_LUT[labels_pred_single])\n",
        "            ax2.set_xlim([-25, 25])\n",
        "            ax2.set_ylim([0, 40])\n",
        "\n",
        "        space_threshold = 1\n",
        "        speed_threshold = [0, 3, 2, 1.2]\n",
        "        score_threshold = 0.6\n",
        "\n",
        "        if post_clst_id_single.shape[0] > 0 and filter_objects:\n",
        "            min_dist_mat = 10 * np.ones([int(post_clst_id_single.max() + 1), int(post_clst_id_single.max() + 1)])\n",
        "            min_v_diff_mat = 10 * np.ones([int(post_clst_id_single.max() + 1), int(post_clst_id_single.max() + 1)])\n",
        "            object_label_list = 5 * np.ones(int(post_clst_id_single.max() + 1))\n",
        "            for k in np.arange(int(post_clst_id_single.max() + 1)):\n",
        "                for l in np.arange(int(post_clst_id_single.max() + 1)):\n",
        "                    if k!=l and np.sum(post_clst_id_single == k) == 0 or np.sum(post_clst_id_single == l) == 0:\n",
        "                        continue\n",
        "                    object_label_list[k] = labels_pred_single[post_clst_id_single==k][0]\n",
        "                    min_dist_pair = distance.cdist(targets_xyv_single[post_clst_id_single == k, :2], targets_xyv_single[post_clst_id_single == l, :2]).min()\n",
        "                    min_dist_mat[k, l] = min_dist_pair \n",
        "                    min_v_diff_pair = distance.cdist(np.reshape(targets_xyv_single[post_clst_id_single == k, 2], (-1, 1)), np.reshape(targets_xyv_single[post_clst_id_single == l, 2], (-1, 1))).min()\n",
        "                    min_v_diff_mat[k, l] = min_v_diff_pair\n",
        "                    min_score_diff_pair = distance.cdist(targets_scores_single[post_clst_id_single == k, :], targets_scores_single[post_clst_id_single == l, :]).min()\n",
        "                    if min_dist_pair < space_threshold:\n",
        "                        label1 = labels_pred_single[post_clst_id_single == k][0] \n",
        "                        label2 = labels_pred_single[post_clst_id_single == l][0]\n",
        "                        if (label1 == 2 and label2 == 3) or (label1 == 3 and label2 == 2):\n",
        "                            min_scores_list.append(min_score_diff_pair)\n",
        "                            # print(min_scores_list)\n",
        "                            if min_v_diff_pair < speed_threshold[3]:\n",
        "                                num_targets1 = np.sum(post_clst_id_single == k)\n",
        "                                num_targets2 = np.sum(post_clst_id_single == l)\n",
        "                                if num_targets1 > num_targets2:\n",
        "                                    label_refine = label1\n",
        "                                else:\n",
        "                                    label_refine = label2 \n",
        "                                if label_refine == 3 and min_score_diff_pair < score_threshold:\n",
        "                                    post_clst_id_single[post_clst_id_single == k] = l\n",
        "                                    labels_pred_single[post_clst_id_single == k] = label_refine\n",
        "                                    labels_pred_single[post_clst_id_single == l] = label_refine\n",
        "                        elif (label1 == 1 and label2 == 3) or (label1 == 3 and label2 == 1):\n",
        "                            if min_v_diff_pair < speed_threshold[3]:\n",
        "                                num_targets1 = np.sum(post_clst_id_single == k)\n",
        "                                num_targets2 = np.sum(post_clst_id_single == l)\n",
        "                                if num_targets1 > num_targets2:\n",
        "                                    label_refine = label1\n",
        "                                else:\n",
        "                                    label_refine = label2 \n",
        "                                if label_refine == 3  and min_score_diff_pair < score_threshold:\n",
        "                                    post_clst_id_single[post_clst_id_single == k] = l\n",
        "                                    labels_pred_single[post_clst_id_single == k] = label_refine\n",
        "                                    labels_pred_single[post_clst_id_single == l] = label_refine\n",
        "                        elif (label1 == 1 and label2 == 2) or (label1 == 2 and label2 == 1):\n",
        "                            if min_v_diff_pair < speed_threshold[2]:\n",
        "                                num_targets1 = np.sum(post_clst_id_single == k)\n",
        "                                num_targets2 = np.sum(post_clst_id_single == l)\n",
        "                                if num_targets1 > num_targets2:\n",
        "                                    label_refine = label1\n",
        "                                else:\n",
        "                                    label_refine = label2 \n",
        "                                if label_refine == 2 and min_score_diff_pair < score_threshold:\n",
        "                                    post_clst_id_single[post_clst_id_single == k] = l\n",
        "                                    labels_pred_single[post_clst_id_single == k] = label_refine\n",
        "                                    labels_pred_single[post_clst_id_single == l] = label_refine\n",
        "            num_obj_around = np.sum(np.logical_and(min_dist_mat < space_threshold, min_v_diff_mat < speed_threshold[3]), axis = 1)\n",
        "            id_list_of_cars_surrounded_by_a_lot_of_bikers = np.nonzero(np.logical_and(num_obj_around>2, object_label_list == 3))\n",
        "            for id_of_cars_surrounded_by_a_lot_of_bikers in id_list_of_cars_surrounded_by_a_lot_of_bikers:\n",
        "                labels_pred_single[post_clst_id_single == id_of_cars_surrounded_by_a_lot_of_bikers] = 2\n",
        "\n",
        "        # second time                     \n",
        "        targets_xyv_ped = targets_xyv_single[labels_pred_single==1, :]\n",
        "        targets_xyv_biker = targets_xyv_single[labels_pred_single==2, :]\n",
        "        targets_xyv_car = targets_xyv_single[labels_pred_single==3, :]\n",
        "        if targets_xyv_ped.shape[0] > 0:\n",
        "            post_clst_id_ped = cylinder_cluster(targets_xyv_ped[:, :3], eps_xy = eps_xy_list[1], eps_v = eps_v_list[1], min_targets=min_targets_list[1])\n",
        "            max_clst_id_ped = post_clst_id_ped.max()\n",
        "        else:\n",
        "            post_clst_id_ped = -1 * np.ones([targets_xyv_ped.shape[0]])\n",
        "            max_clst_id_ped = -1\n",
        "        if targets_xyv_biker.shape[0] > 1:\n",
        "            post_clst_id_biker = cylinder_cluster(targets_xyv_biker[:, :3], eps_xy = eps_xy_list[2], eps_v = eps_v_list[2], min_targets=min_targets_list[2] )\n",
        "            max_clst_id_biker = max_clst_id_ped + 1 + post_clst_id_biker.max()\n",
        "        else:\n",
        "            post_clst_id_biker = -1 * np.ones([targets_xyv_biker.shape[0]])\n",
        "            max_clst_id_biker = max_clst_id_ped \n",
        "        if targets_xyv_car.shape[0] > 2:\n",
        "            post_clst_id_car = cylinder_cluster(targets_xyv_car[:, :3], eps_xy = eps_xy_list[3], eps_v = eps_v_list[3], min_targets=min_targets_list[3])\n",
        "            \n",
        "        else:\n",
        "            post_clst_id_car = -1 * np.ones([targets_xyv_car.shape[0]])\n",
        "\n",
        "        post_clst_id_biker[post_clst_id_biker>-1] = post_clst_id_biker[post_clst_id_biker>-1] + max_clst_id_ped+1\n",
        "        post_clst_id_car[post_clst_id_car>-1] = post_clst_id_car[post_clst_id_car>-1] + max_clst_id_biker + 1\n",
        "\n",
        "        post_clst_id_single = -1 * np.ones(np.sum(frame_id == i))\n",
        "        post_clst_id_single[labels_pred_single==1] = post_clst_id_ped \n",
        "        post_clst_id_single[labels_pred_single==2] = post_clst_id_biker\n",
        "        post_clst_id_single[labels_pred_single==3] = post_clst_id_car\n",
        "        labels_pred_single[post_clst_id_single==-1] = 0\n",
        "\n",
        "        post_clst_id[frame_id==i] = post_clst_id_single\n",
        "        labels_pred[frame_id==i] = labels_pred_single\n",
        "\n",
        "    min_scores_list = np.array(min_scores_list)\n",
        "    return post_clst_id, labels_pred \n",
        "def cal_f1(precision, recall):\n",
        "\n",
        "    return 2*precision*recall/(precision+recall)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Test the post_clustering->precision & recall pipeline \"\"\"\n",
        "    BASE_DIR = '/content/radar_cnn'\n",
        "    result_DIR = osp.join(BASE_DIR, 'results', 'RTCtrain_info')\n",
        "    config_list = os.listdir(result_DIR)\n",
        "    config_list.sort()\n",
        "    RTCnet_result_info_path = osp.join(result_DIR, config_list[-1])\n",
        "    speed_threshold = 0.3\n",
        "    speed_threshold_to_change_label = 0\n",
        "    show_RTC_result = True\n",
        "    with open(RTCnet_result_info_path, 'r') as f:\n",
        "        RTCnet_result_info = json.load(f)\n",
        "\n",
        "\n",
        "    if show_RTC_result:\n",
        "        result_path = RTCnet_result_info[\"result_folder\"]\n",
        "\n",
        "        data_path = RTCnet_result_info[\"data_path\"]\n",
        "       \n",
        "        labels_pred = np.load(osp.join(result_path, \"pred_labels_test.npy\"))\n",
        "        labels_true = np.load(osp.join(result_path, \"true_labels_test.npy\"))\n",
        "        target_scores = np.load(osp.join(result_path, \"final_score.npy\"))\n",
        "        frame_id = np.load(osp.join(data_path, \"test\", \"frame_id.npy\"))\n",
        "        instance_id_true = np.load(osp.join(data_path, \"instance_id_test.npy\"))\n",
        "        targets_rav = np.load(osp.join(data_path, \"test\", \"features.npy\"))[:, :3]\n",
        "\n",
        "\n",
        "        targets_xyv = np.zeros(targets_rav.shape)\n",
        "        targets_xyv[:,0] = targets_rav[:,0]*np.cos(targets_rav[:,1])\n",
        "        targets_xyv[:,1] = targets_rav[:,0]*np.sin(targets_rav[:,1])\n",
        "        targets_xyv[:,2] = targets_rav[:,2]\n",
        "        targets_v = np.abs(targets_rav[:, 2])\n",
        "        labels_pred[targets_v < speed_threshold_to_change_label] = 0\n",
        "        labels_pred = labels_pred[targets_v > speed_threshold]\n",
        "        labels_true = labels_true[targets_v > speed_threshold]\n",
        "        target_scores = target_scores[targets_v > speed_threshold]\n",
        "        frame_id = frame_id[targets_v > speed_threshold]\n",
        "        targets_rav = targets_rav[targets_v>speed_threshold, :]\n",
        "        targets_xyv = targets_xyv[targets_v > speed_threshold, :]\n",
        "        \n",
        "        instance_id_true = instance_id_true[targets_v > speed_threshold]\n",
        "        instance_id_pred, labels_pred = post_clustering(targets_xyv, labels_pred, frame_id, target_scores = target_scores)\n",
        "        precision_RTC, recall_RTC, IoU_RTC, num_TP_single_target_total = cal_precision_recall_all(labels_true, labels_pred, instance_id_true, instance_id_pred, frame_id, targets_for_debug=targets_xyv)\n",
        " \n",
        "        print(\"precision_RTC\", precision_RTC, \"recall_RTC\", recall_RTC)\n",
        "        np.savetxt(osp.join(result_path, \"precision_RTC.csv\"), precision_RTC, delimiter=\",\")\n",
        "        np.savetxt(osp.join(result_path, \"recall_RTC.csv\"), recall_RTC, delimiter=\",\")\n",
        "        np.savetxt(osp.join(result_path, \"F1score.csv\"), cal_f1(precision_RTC, recall_RTC), delimiter=\",\")\n",
        "        np.savetxt(osp.join(result_path, \"IoU_RTC.csv\"), IoU_RTC, delimiter=\",\")\n",
        "        np.save(osp.join(result_path, \"pred_labels_refine.npy\"), labels_pred)\n",
        "        print(\"number of TP object with 1 target:\", num_TP_single_target_total)\n",
        "        print(\"f1 RTC: \", cal_f1(precision_RTC, recall_RTC))\n",
        "        print(\"IoU RTC:\", IoU_RTC)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BTF5ijy2llqw",
        "G8Mk31W8zsVa",
        "-Zu0_8_Hr9VE",
        "xRGjJqKXsNpK"
      ],
      "name": "radar2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPeaJ9qzKarcwGiCGKKatx2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}